# Video 1 ‚Äì Introduction to LangGraph & Agentic AI Playlist

## What is this playlist about?

* A new YouTube playlist focused on **Agentic AI using LangGraph**.
* The creator received many requests to explain LangGraph, so he decided to teach it in a structured way.
* He spent **3‚Äì4 months researching and preparing** the curriculum.

---

## Why this Playlist?

There are **3 main reasons**:

### **1Ô∏è‚É£ Timing**

* Right now is the perfect time to learn **Agentic AI**.
* Everywhere‚ÄîYouTube, Twitter, Instagram‚Äîyou will see the term **‚ÄúAgentic AI‚Äù**.
* Industry leaders believe Agentic AI will be the **next big thing** in computer science.
* Generative AI tools like **ChatGPT** are mature enough now, enabling powerful AI agents in the coming years.

### **2Ô∏è‚É£ High Demand**

* Viewers repeatedly asked for a **LangGraph playlist**.
* There is huge interest in this topic across the industry.

### **3Ô∏è‚É£ Progressive Learning**

The channel has already covered:

* **Machine Learning**
* **Deep Learning**
* **LangChain and Generative AI**

Now students are ready to learn **LangGraph and Agentic AI**.

---

## Vision Behind This Playlist

The creator has **three goals** for learners:

### ‚≠ê Goal 1: Easy Learning

* Even beginners should be able to build **Agentic AI applications**.

### ‚≠ê Goal 2: Strong Fundamentals

* Gain a **strong command** over LangGraph concepts.

### ‚≠ê Goal 3: Conceptual Understanding

* Even if LangGraph is replaced by another tool, you should be able to learn the new one easily.

---

## Curriculum (Tentative Modules)

> *Note: May change as Agentic AI is evolving fast.*

### **Module 1 ‚Äì Foundations of Agentic AI**

* What is Agentic AI?
* Difference between:

  * Agentic AI vs AI Agent
  * Agentic AI vs Generative AI
  * Traditional RAG vs Agentic RAG
* Frameworks for building agents

### **Module 2 ‚Äì LangGraph Fundamentals**

* What is a LangGraph?
* Key concepts:

  * Graph
  * State
  * Nodes
  * Edges
  * Conditional edges
  * Loops
* Build **basic AI workflows** using LangGraph

### **Module 3 ‚Äì Advanced LangGraph Concepts**

* Persistence
* Memory
* Human-in-the-loop
* Breakpoints
* Checkpoints
* Time travel
* Build **industry-level agents**

### **Module 4 ‚Äì Building AI Agents**

Understand design patterns such as:

* ReAct
* Reflection
* Self-Ask with Search
* Planning
* Multi-agent systems

### **Module 5 ‚Äì Agentic RAG**

* Combine **Agents + RAG**
* Architectures:

  * C-RAG
  * Self-RAG
* Build advanced RAG systems using agents

### **Module 6 ‚Äì Production & Deployment**

* Build a **real project**
* Add:

  * UI
  * Debugging tools
  * Observability
  * LangSmith integration
* Deploy and showcase in **resume/interviews**

---

## Prerequisites

To follow the playlist smoothly, you should know:

### ‚úî Python (Intermediate Level)

* OOP concepts
* Typing module
* Pydantic
* Async IO

### ‚úî Basics of LLMs

### ‚úî LangChain

* LangGraph is built on top of LangChain
* The creator recommends watching the **LangChain playlist first**

# Generative AI vs Agentic AI ‚Äî Video 2

## 1. Introduction

This playlist teaches Agentic AI using LangGraph. Before learning Agentic AI, it‚Äôs important to understand how it differs from Generative AI.

## 2. What is Generative AI?

Generative AI (GenAI) refers to AI models that can create new content such as:

* Text (ChatGPT, Gemini, Claude)
* Images (DALL¬∑E, Midjourney)
* Code (Code Llama)
* Speech (ElevenLabs)
* Videos (Sora, Runway ML)

### Simple Definition

Generative AI models learn from existing data and create new data that looks like it was created by humans.

### How GenAI Works

* It does not focus on input-output relationships like traditional AI.
* It learns the pattern and distribution of data.
* It generates new samples from that learned distribution.

### Example

Give GenAI thousands of cat images ‚Üí it learns what a cat looks like ‚Üí generates a new cat image.

## 3. Traditional AI vs Generative AI

| Feature     | Traditional AI                         | Generative AI             |
| ----------- | -------------------------------------- | ------------------------- |
| Purpose     | Predict or classify                    | Create new content        |
| Learning    | Finds relations between input & output | Learns data distribution  |
| Examples    | Spam detection, cancer detection       | ChatGPT, Midjourney       |
| Output Type | Label or number                        | Text, Image, Audio, Video |

## 4. Applications of Generative AI

* Creative Writing: blogs, email drafting
* Software Development: code generation, debugging
* Customer Support: chatbots
* Education: learning assistance
* Design: thumbnails, infographics, AI-generated ads

## 5. Scenario ‚Äî HR Hiring Workflow

**Goal:** Hire a Backend Engineer

### Hiring Steps

1. Write Job Description (JD)
2. Post job on platforms like LinkedIn
3. Shortlist candidates
4. Schedule interviews
5. Conduct interviews
6. Send offer letter
7. Onboard the candidate

## 6. Four Levels of AI Solutions

### 1Ô∏è‚É£ Simple Generative AI Chatbot

**Capabilities:** Helps draft JD, emails, interview questions
**Problems:**

* Reactive (waits for input)
* No memory
* Generic advice
* Cannot take actions

### 2Ô∏è‚É£ RAG-based Chatbot (Retrieval Augmented Generation)

**Improvement:** Connect to company documents like:

* Old JDs, hiring strategies, onboarding policies
  **Benefits:**
* Company-specific answers
  **Limitation:** Still reactive, cannot act independently

### 3Ô∏è‚É£ Tool-Augmented Chatbot

**Improvements:** Connect with tools such as:

* LinkedIn API, resume parser, email sender, calendar tools
  **Benefits:**
* Can take real actions (post jobs, send emails, schedule interviews)
  **Limitation:** Still not proactive

### 4Ô∏è‚É£ Agentic AI (Final Stage)

**Capabilities:**

* Understands the goal
* Plans steps automatically
* Executes autonomously
* Has memory & context
* Adapts to changes
* Makes decisions

**Example Behavior:**

* Notices low applications ‚Üí updates JD + boosts LinkedIn post
* Tracks onboarding and notifies only when approval is needed

## 7. Key Differences ‚Äî Generative AI vs Agentic AI

| Feature            | Generative AI  | Agentic AI                     |
| ------------------ | -------------- | ------------------------------ |
| Goal               | Create content | Achieve a goal end-to-end      |
| Nature             | Reactive       | Proactive & autonomous         |
| Needs user prompt? | Yes            | Only once                      |
| Uses tools?        | Optional       | Mandatory                      |
| Memory             | Not always     | Yes                            |
| Planning           | No             | Yes                            |
| Adaptability       | No             | Yes                            |
| Components         | Only LLM       | LLM + Memory + Tools + Planner |

## 8. One-Line Understanding

**Generative AI = capability** ‚Üí creates content
**Agentic AI = behavior** ‚Üí achieves goals autonomously using GenAI + tools + memory + planning

## 9. Conclusion

Generative AI started the revolution, but Agentic AI is the next big leap where AI becomes:

* Autonomous
* Context-aware
* Action-driven
* Goal-oriented

Agentic AI doesn't just answer ‚Äî it acts, plans, and completes tasks like a real digital employee.


# Agentic AI ‚Äì Video 3

## What is Agentic AI?

Agentic AI is a type of AI that:

* Takes a goal from a user
* Plans how to achieve it
* Executes tasks automatically
* Adapts when situations change
* Asks for help only when needed

‚û°Ô∏è It does things proactively, unlike normal chatbots which are only reactive.

---

### Example Difference

| Generative AI (ChatGPT)         | Agentic AI                                        |
| ------------------------------- | ------------------------------------------------- |
| Answers only what you ask       | Takes your goal and completes the entire workflow |
| Reactive                        | Proactive                                         |
| No planning                     | Makes plans and executes                          |
| Needs instructions step-by-step | Works independently                               |

---

## Example

**Goal:** Hire a Backend Engineer

Agentic AI will:

* Understand the goal
* Create a job description (JD)
* Post on job portals
* Monitor applicants
* Modify strategy if no applications
* Parse resumes, shortlist candidates
* Schedule interviews using calendar
* Send offer letters
* Onboard selected candidate

‚û°Ô∏è You gave one goal ‚Üí AI handled everything end-to-end.

---

## Key Characteristics of Agentic AI (6 Traits)

### 1. Autonomy

* Works independently without step-by-step human commands
* Takes actions on its own
* Shows autonomy in:

  * Execution
  * Decision-making
  * Tool selection

**Control Mechanisms:**

* Permission limits
* Human approval checkpoints
* Guardrails (policies)

### 2. Goal-Oriented

* Always works toward a defined goal
* Stores goals in memory
* Handles:

  * Plain goals ‚Üí "Hire a backend engineer"
  * Goals with constraints ‚Üí "Hire remotely, 2‚Äì4 years experience"
* Goals can be updated midway

### 3. Planning

Agentic AI works in two phases:

```
PLAN ‚Üí EXECUTE ‚Üí REPLAN (if something fails) ‚Üí EXECUTE
```

Planning steps:

* Generate multiple plans
* Evaluate plans (cost, risk, tools, constraints)
* Select the best plan

### 4. Reasoning

Used in planning and execution to:

* Understand goal
* Break tasks into steps
* Decide tools to use
* Handle errors
* Decide when to involve human

**Reasoning = thinking logic of the agent**

### 5. Adaptability

Changes strategy if:

* Something fails (e.g., API is down)
* External feedback changes (low applicants)
* Goal changes midway

‚û°Ô∏è Agent modifies actions without restarting.

### 6. Context Awareness

Agent remembers:

* The main goal
* Task progress
* Past chats
* Tool responses
* User preferences
* Guardrails

**Memory Types:**

* Short-term memory ‚Üí current session data
* Long-term memory ‚Üí policies, user preferences, history

‚û°Ô∏è Without context, an agent cannot operate across multiple days or steps.

---

## Core Components of Agentic AI (5 Components)

| Component        | Function                                                            |
| ---------------- | ------------------------------------------------------------------- |
| **Brain (LLM)**  | Understands goals, plans, reasons, selects tools, communicates      |
| **Orchestrator** | Executes steps in correct order, handles loops, conditions, retries |
| **Tools**        | External actions: APIs, email, database, search, RAG                |
| **Memory**       | Stores short-term and long-term information                         |
| **Supervisor**   | Human approval system, applies rules, handles escalations           |

---

## Summary

```
Agentic AI = Goal + Planning + Reasoning + Autonomy + Tools + Memory + Supervisor
```

It behaves like a **digital employee** capable of executing full workflows, not just answering questions.

---

## How to Identify Agentic AI

Ask these 6 questions:

| Question                  | If YES ‚Üí Agentic |
| ------------------------- | ---------------- |
| Is it autonomous?         | ‚úîÔ∏è               |
| Is it goal-driven?        | ‚úîÔ∏è               |
| Does it plan steps?       | ‚úîÔ∏è               |
| Does it reason?           | ‚úîÔ∏è               |
| Can it adapt?             | ‚úîÔ∏è               |
| Does it remember context? | ‚úîÔ∏è               |

‚û°Ô∏è If all are YES ‚Üí It‚Äôs Agentic AI.

# Agentic AI using LangGraph 

## 1. video 4

* This is the **4rd video** in the *Agentic AI using LangGraph* playlist.
* Previous videos covered:

  * Difference between **Generative AI** and **Agentic AI**
  * What **Agentic AI** is, with a real example (Automated Hiring)
  * Characteristics, traits, and components of Agentic AI
* This video shifts focus to **practical development** of Agentic AI applications.

---

## 2. Why Frameworks Are Needed for Agentic AI

* Agentic AI applications are **complex**.
* You cannot write everything **from scratch in Python**.
* Many frameworks exist, such as:

  * CrewAI
  * Microsoft Autogen
  * Others
* This playlist uses **LangGraph**:

  * Built by the **LangChain team**
  * One of the **top frameworks** for Agentic AI

---

## 3. Goals of This Video

By the end of this video, you will understand:

* Why **LangGraph** exists
* What **LangGraph** is (technical overview)
* **LangChain vs LangGraph**
* When to use **LangChain** and when to use **LangGraph**

---

## 4. Prerequisites

You should already know:

* What **LangChain** is
* Basic LangChain components
* How simple LangChain code works

üëâ If not, first watch:

* LangChain Introduction
* LangChain Components

---

## 5. Quick Recap: What is LangChain?

### Definition

> LangChain is an open-source library that simplifies building LLM-based applications.

### Why LangChain Exists

LLM-based applications require many parts:

* Prompts
* Models
* Retrieval
* Memory
* Tools

LangChain **simplifies and connects** all these parts.

---

## 6. Core LangChain Building Blocks

### 1. Models

* Unified interface for:

  * OpenAI
  * Anthropic (Claude)
  * HuggingFace
  * Ollama
* Easy to switch models **without rewriting code**

### 2. Prompts

* Helps design and manage prompts
* Supports prompt templates and reuse

### 3. Retrievers

* Fetch relevant data from:

  * Vector databases
  * Knowledge bases
* Used for **RAG (Retrieval-Augmented Generation)** applications

### 4. Chains (Most Important)

* Connect components in a **linear sequence**
* Output of one step becomes input of the next step
* Best for:

  * Chatbots
  * Summarizers
  * Simple multi-step workflows

---

## 7. What Can You Build with LangChain?

* Chatbots
* Text summarizers
* Multi-step report generators
* RAG-based systems
* Basic agent-like workflows using tools

‚ö†Ô∏è **Limitation**:

* Works best for **linear workflows**
* Struggles with **complex, non-linear logic**

---

## 8. Workflow vs Agent (Very Important Concept)

### Workflow

* Steps are predefined by the developer
* Executes in the same order every time
* Static

### Agent

* LLM decides:

  * Steps
  * Order
  * Tool usage
* Dynamic and autonomous

üëâ The automated hiring example shown is a **workflow**, not a full agent.

---

## 9. Automated Hiring Workflow (Overview)

Steps include:

* Receive hiring request
* Create job description (JD)
* Human approval
* Post job
* Wait & monitor applications
* Modify JD if needed (loop)
* Shortlist candidates
* Schedule interviews
* Conduct interviews
* Send offers
* Handle acceptance / rejection
* Onboarding

This workflow:

* Has loops
* Has conditions
* Has wait times
* Runs for **days or weeks**

---

## 10. Challenge 1: Control Flow Complexity

### Problem with LangChain

* LangChain chains are **linear**
* This workflow is **non-linear**:

  * Conditional branches
  * Loops
  * Jumps

### Result

* Requires a lot of custom **Python glue code**
* Glue code is:

  * Hard to maintain
  * Hard to debug
  * Error-prone

---

## 11. How LangGraph Solves Control Flow

### Key Idea

* Represent the workflow as a **graph**

  * Nodes = tasks
  * Edges = control flow

### Benefits

* Native support for:

  * Conditions
  * Loops
  * Jumps
* No glue code
* Cleaner and more maintainable logic

---

## 12. Challenge 2: State Handling

### What is State?

All important data during workflow execution, such as:

* Job description
* Approval status
* Application count
* Interview results
* Offer status

State **changes over time**.

### LangChain Problem

* LangChain is **stateless**
* Memory only stores conversation text
* State must be manually managed using dictionaries

---

## 13. LangGraph State Management

* LangGraph is **stateful**
* You define a **state object**
* Every node:

  * Can read state
  * Can update state
* State flows automatically across nodes

üëâ LangChain = Stateless
üëâ LangGraph = Stateful

---

## 14. Challenge 3: Event-Driven Execution

### Two Types of Execution

* **Sequential**: Runs start ‚Üí end without stopping
* **Event-driven**:

  * Pauses
  * Waits for an external event
  * Resumes later

### Hiring Workflow Needs Event-Driven Execution

* Wait 7 days after job posting
* Wait 48 hours after JD modification
* Wait for candidate response
* Wait for human approvals

### LangChain Limitation

* No pause / resume support
* Workflow must be manually broken

### LangGraph Advantage

* Built-in pause & resume
* Uses checkpoints
* Can resume exactly where it stopped

---

## 15. Challenge 4: Fault Tolerance

### Problem

Long-running workflows can fail due to:

* API failure
* Server crash
* Network issues

### LangChain

* If it fails ‚Üí start from the beginning

### LangGraph

* Supports:

  * Retry logic (small failures)
  * Recovery from last checkpoint (big failures)

---

## 16. Challenge 5: Human in the Loop

### Meaning

* Workflow pauses for **human decision**
* Examples:

  * JD approval
  * Offer approval

### LangChain

* Works only for short waits
* Not suitable for long approvals (hours or days)

### LangGraph

* First-class support
* Can pause indefinitely
* Resume after human input

---

## 17. Feature: Nested Workflows (Subgraphs)

### Concept

* A node can itself be a **graph**
* Enables:

  * Modular design
  * Reusability
  * Multi-agent systems

### Example

* ‚ÄúConduct Interview‚Äù can be a full workflow:

  * Question generation
  * Multiple rounds
  * Evaluation

---

## 18. Observability (Monitoring & Debugging)

### Why Important

* Debug errors
* Audit decisions
* Understand agent behavior

### LangChain + LangSmith

* Tracks LangChain code
* ‚ùå Cannot track glue code

### LangGraph + LangSmith

* Full visibility
* Tracks:

  * Node execution
  * State changes
  * Human inputs
  * Decision paths

---

## 19. Final Summary

### What is LangGraph?

> LangGraph is an orchestration framework to build **stateful, multi-step, event-driven workflows** using LLMs.

Think of LangGraph as:

> **A flowchart engine for LLMs**

---

## 20. When to Use What?

### Use LangChain when:

* Simple linear flows
* Chatbots
* Summarizers
* Basic RAG

### Use LangGraph when:

* Complex workflows
* Conditions & loops
* Human-in-the-loop
* Event-driven execution
* Long-running systems
* Multi-agent systems

---

## 21. Important Note

* LangGraph does **not** replace LangChain
* LangGraph is built **on top of LangChain**
* You still use from LangChain:

  * Prompts
  * Models
  * Retrievers
  * Tools

üëâ **LangChain = Components**
üëâ **LangGraph = Orchestration**

# üìò Agentic AI using LangGraph ‚Äì Simple English Notes (Video 5)

---

## 1. About This Video

* This is the **4th video** in the *Agentic AI using LangGraph* playlist.
* The previous video covered **LangChain vs LangGraph** comparison.
* This video is **fully dedicated to LangGraph core concepts**.

### üéØ Goal

* Understand concepts first ‚Üí **coding becomes easy** in the next videos.

---

## 2. What is LangGraph? (Quick Revision)

### Definition

**LangGraph is an orchestration framework for LLM workflows.**

### In Simple Words

* You give an **LLM workflow** to LangGraph.
* LangGraph converts that workflow into a **graph**.
* LangGraph then **executes the workflow automatically**.

---

## 3. Why ‚ÄúGraph‚Äù?

### Graph Structure

* **Nodes** ‚Üí Tasks
* **Edges** ‚Üí Execution flow (what runs next)

### Example Tasks (Nodes)

* Call an LLM
* Call a tool
* Make a decision
* Store memory

### What Edges Tell

* After this task ‚Üí **which task should run next**

---

## 4. Key Features of LangGraph

LangGraph supports:

* ‚úÖ Sequential execution
* ‚úÖ Parallel execution
* ‚úÖ Branching (if / else)
* ‚úÖ Loops (retry, iteration)
* ‚úÖ Memory (conversation / state)
* ‚úÖ Resume from failure

üëâ Because of this, LangGraph is **ideal for Agentic and Production-grade AI systems**.

---

## 5. What is an LLM Workflow?

### Workflow (General)

A workflow is:

> A series of tasks executed in order to achieve a goal.

**Example ‚Äì Hiring Workflow**

* Create JD
* Post job
* Shortlist candidates
* Interview
* Onboard

### LLM Workflow

An **LLM workflow** is a workflow where:

* Many steps depend on **LLMs**

**Examples**

* JD writing ‚Üí LLM
* Resume shortlisting ‚Üí LLM
* Interview Q&A ‚Üí LLM

---

## 6. Common LLM Workflow Patterns

### 1Ô∏è‚É£ Prompt Chaining

**Idea:**
Break a complex task into smaller LLM calls.

**Example ‚Äì Report Generation**

* LLM ‚Üí Create outline
* LLM ‚Üí Write detailed report from outline
* Add checks (word limit, quality)

‚úÖ Easy to debug
‚úÖ Easy to validate intermediate outputs

---

### 2Ô∏è‚É£ Routing

**Idea:**
One LLM decides **which expert should handle the task**.

**Example ‚Äì Customer Support Bot**

* Refund query ‚Üí Refund LLM
* Technical query ‚Üí Tech LLM
* Sales query ‚Üí Sales LLM

üëâ One LLM acts as a **decision maker (router)**.

---

### 3Ô∏è‚É£ Parallelization

**Idea:**
Run multiple tasks **at the same time**, then merge results.

**Example ‚Äì YouTube Content Moderation**

* Check community guidelines
* Check misinformation
* Check sexual content

All checks run **in parallel**, results are merged ‚Üí final decision.

---

### 4Ô∏è‚É£ Orchestrator‚ÄìWorker Pattern

**Idea:**

* Tasks are **not predefined**
* Orchestrator LLM decides tasks dynamically

**Example ‚Äì Research Assistant**

* Scientific query ‚Üí Google Scholar
* Political query ‚Üí Google News
* Social topic ‚Üí Blogs / Articles

üëâ Orchestrator assigns work to workers **based on input**.

---

### 5Ô∏è‚É£ Evaluator‚ÄìOptimizer

**Idea:**
Improve output through **iteration + feedback loop**.

**Two LLMs**

* Generator ‚Üí Creates output
* Evaluator ‚Üí Checks quality and gives feedback

**Examples**

* Blog writing
* Email drafting
* Story / poem writing

The process repeats until the evaluator is satisfied.

---

## 7. Graphs, Nodes & Edges (Most Important Concept)

### Node

* Represents **one task**
* Behind the scenes ‚Üí **Python function**

### Edge

* Connects nodes
* Defines **execution order**

### Types of Edges

* Sequential
* Parallel
* Conditional (if / else)
* Looping

üëâ **Nodes = What to do**
üëâ **Edges = When to do**

---

## 8. Example: UPSC Essay Evaluation Workflow

### High-Level Flow

1. Generate essay topic
2. User writes essay
3. Evaluate essay:

   * Clarity
   * Depth
   * Language
4. Calculate score
5. If score ‚â• threshold ‚Üí Success
6. Else ‚Üí Feedback + Retry loop

### Graph View

* Each step = Node
* Retry = Loop edge
* Pass / Fail = Conditional edge

---

## 9. What is State in LangGraph?

### Definition

**State = shared memory flowing through the graph**

### What State Contains

* User input
* Scores
* Intermediate results
* Final output

### Key Properties

* Shared by all nodes
* Mutable (can change)
* Evolves over time

### Technical Form

* Usually a **TypedDict (Python)**
* Sometimes a **Pydantic model**

---

## 10. How State Works Internally

1. State is passed to the first node
2. Node updates part of the state
3. Updated state goes to the next node
4. Process continues until the workflow ends

---

## 11. Reducers (Very Important)

### Problem

If multiple nodes update the same state key:

* Replace value?
* Add value?
* Merge value?

### Reducer Decides

**How updates are applied to state**

### Examples

* Replace ‚Üí Overwrite old value
* Add ‚Üí Append to list
* Merge ‚Üí Combine results

### Why Reducers Matter

**Chatbot Example**

* Replace messages ‚Üí Old chats lost ‚ùå
* Append messages ‚Üí Full conversation kept ‚úÖ

**UPSC Essay Example**

* Store all attempts
* Track improvement over time

---

## 12. LangGraph Execution Model (Behind the Scenes)

Inspired by **Google Pregel** (large-scale graph processing).

### Execution Steps

#### 1Ô∏è‚É£ Graph Definition

* Define nodes
* Define edges
* Define state

#### 2Ô∏è‚É£ Compile

* Validate graph structure
* Check orphan nodes
* Ensure logical correctness

#### 3Ô∏è‚É£ Invoke

* Pass initial state to the first node

---

### Message Passing

* State moves through edges
* Nodes update state partially

### Supersteps

* One superstep may contain:

  * One node execution
  * OR multiple parallel node executions

Used because **parallel nodes run together**.

### When Execution Stops

* No active nodes
* No messages flowing

---

## 13. Why This Matters

* You don‚Äôt manually call functions
* LangGraph automatically handles:

  * Execution order
  * Parallelism
  * State passing
  * Loops
  * Failures

---

## 14. Final Takeaways

* **LangGraph** = Graph-based LLM orchestration
* **Nodes** = Python functions
* **Edges** = Execution logic
* **State** = Shared, evolving memory
* **Reducers** = State update rules
* **Execution** = Automatic and scalable

üëâ **Next Video:** First practical LangGraph workflow üöÄ

# üìò Agentic AI using LangGraph ‚Äì Sequential Workflows (Simple Notes)

---

## 1. What this video is about

This video is the **first practical video** in the *Agentic AI using LangGraph* playlist.

### Before this video, we covered (Theory):

* Difference between **Generative AI vs Agentic AI**
* What **Agentic AI** is (deep conceptual understanding)
* Why **LangGraph** is needed when **LangChain** already exists
* Core concepts of **LangGraph**

üëâ Now, theory is complete.
üëâ From this video onwards, **hands-on coding starts**.

---

## 2. Goal of this video

This video has **two main goals**:

1. Teach you **basic LangGraph syntax** (your first LangGraph code)
2. Enable you to build **any simple sequential workflow** on your own

---

## 3. What is a Sequential Workflow?

A **Sequential Workflow** means:

* Tasks run **one after another**
* No branching
* No parallel paths

**Example:**

```
Task 1 ‚Üí Task 2 ‚Üí Task 3 ‚Üí END
```

This video covers:

* Simple **non-LLM workflow**
* Simple **LLM workflow**
* **Prompt chaining** workflow

---

## 4. Installation & Setup

### Step 1: Create a project folder

```
LangGraph_Tutorials/
```

Open this folder in **VS Code**.

---

### Step 2: Create virtual environment

```bash
python -m venv myenv
```

Activate it (Windows):

```bash
myenv\Scripts\activate
```

---

### Step 3: Install required libraries

```bash
pip install langgraph langchain langchain-openai python-dotenv
```

**Why these libraries?**

* `langgraph` ‚Üí Workflow engine
* `langchain` ‚Üí LLM tools (models, prompts, loaders)
* `langchain-openai` ‚Üí OpenAI models
* `python-dotenv` ‚Üí Environment variables

---

## 5. Why Jupyter Notebook?

We use **Jupyter Notebook (.ipynb)** because:

* LangGraph graphs can be **visualized**
* Easier debugging
* Better learning experience

---

## 6. First Workflow (Non-LLM) ‚Äì BMI Calculator

### Problem

* **Input:** weight, height
* **Process:** Calculate BMI
* **Output:** Updated state

---

## 7. Core LangGraph Concept

Every LangGraph workflow follows **4 steps**:

1. Define **State**
2. Add **Nodes**
3. Add **Edges**
4. **Compile & Execute**

---

## 8. Define State (TypedDict)

State = shared memory that flows through the graph.

```python
from typing import TypedDict

class BMIState(TypedDict):
    weight_kg: float
    height_m: float
    bmi: float
```

---

## 9. Create the Graph

```python
from langgraph.graph import StateGraph

graph = StateGraph(BMIState)
```

---

## 10. Define Node (Python Function)

Each **node = Python function**

```python
def calculate_bmi(state: BMIState) -> BMIState:
    weight = state["weight_kg"]
    height = state["height_m"]

    bmi = weight / (height ** 2)
    state["bmi"] = round(bmi, 2)

    return state
```

---

## 11. Add Node to Graph

```python
graph.add_node("calculate_bmi", calculate_bmi)
```

---

## 12. Add Start & End Edges

```python
from langgraph.graph import START, END

graph.add_edge(START, "calculate_bmi")
graph.add_edge("calculate_bmi", END)
```

---

## 13. Compile the Graph

```python
workflow = graph.compile()
```

---

## 14. Execute the Workflow

```python
initial_state = {
    "weight_kg": 80,
    "height_m": 1.73
}

final_state = workflow.invoke(initial_state)
print(final_state)
```

**Output:**

```python
{
 'weight_kg': 80,
 'height_m': 1.73,
 'bmi': 26.73
}
```

---

## 15. Visualize the Graph (Jupyter Only)

```python
workflow.get_graph().draw_mermaid_png()
```

Graph:

```
START ‚Üí calculate_bmi ‚Üí END
```

---

## 16. Making Workflow Slightly Complex (BMI Category)

Now we add:

* A **second node**
* BMI category: Underweight / Normal / Overweight / Obese

---

## 17. Update State

```python
class BMIState(TypedDict):
    weight_kg: float
    height_m: float
    bmi: float
    category: str
```

---

## 18. New Node ‚Äì Label BMI

```python
def label_bmi(state: BMIState) -> BMIState:
    bmi = state["bmi"]

    if bmi < 18.5:
        state["category"] = "Underweight"
    elif bmi < 25:
        state["category"] = "Normal"
    elif bmi < 30:
        state["category"] = "Overweight"
    else:
        state["category"] = "Obese"

    return state
```

---

## 19. Add Node & Edges

```python
graph.add_node("label_bmi", label_bmi)

graph.add_edge("calculate_bmi", "label_bmi")
graph.add_edge("label_bmi", END)
```

Graph:

```
START ‚Üí calculate_bmi ‚Üí label_bmi ‚Üí END
```

---

## 20. Execute Again

```python
final_state = workflow.invoke({
    "weight_kg": 80,
    "height_m": 1.73
})

print(final_state)
```

**Output:**

```python
{
 'weight_kg': 80,
 'height_m': 1.73,
 'bmi': 26.73,
 'category': 'Overweight'
}
```

---

# üöÄ LLM Workflow (LangGraph + LangChain)

## 21. LLM Workflow Idea

* Input: **question**
* Node calls **LLM**
* Output: **answer**

---

## 22. Environment Setup

Create a `.env` file:

```
OPENAI_API_KEY=your_api_key_here
```

---

## 23. Imports

```python
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from typing import TypedDict
from dotenv import load_dotenv

load_dotenv()
model = ChatOpenAI()
```

---

## 24. Define State

```python
class LLMState(TypedDict):
    question: str
    answer: str
```

---

## 25. Node Function

```python
def llm_qa(state: LLMState) -> LLMState:
    question = state["question"]

    prompt = f"Answer the following question:\n{question}"
    response = model.invoke(prompt)

    state["answer"] = response.content
    return state
```

---

## 26. Build Graph

```python
graph = StateGraph(LLMState)

graph.add_node("llm_qa", llm_qa)
graph.add_edge(START, "llm_qa")
graph.add_edge("llm_qa", END)

workflow = graph.compile()
```

---

## 27. Execute

```python
final_state = workflow.invoke({
    "question": "How far is the Moon from Earth?"
})

print(final_state["answer"])
```

---

# üîó Prompt Chaining Workflow

Prompt chaining = **multiple LLM calls in sequence**

---

## 28. Prompt Chaining Use Case

* Generate **outline**
* Generate **blog using outline**

---

## 29. State Definition

```python
class BlogState(TypedDict):
    title: str
    outline: str
    content: str
```

---

## 30. Node 1 ‚Äì Create Outline

```python
def create_outline(state: BlogState) -> BlogState:
    title = state["title"]

    prompt = f"Generate a detailed outline for a blog on the topic: {title}"
    response = model.invoke(prompt)

    state["outline"] = response.content
    return state
```

---

## 31. Node 2 ‚Äì Create Blog

```python
def create_blog(state: BlogState) -> BlogState:
    title = state["title"]
    outline = state["outline"]

    prompt = f"""
    Write a detailed blog on the topic \"{title}\"
    using the following outline:
    {outline}
    """

    response = model.invoke(prompt)
    state["content"] = response.content
    return state
```

---

## 32. Build Prompt Chaining Graph

```python
graph = StateGraph(BlogState)

graph.add_node("create_outline", create_outline)
graph.add_node("create_blog", create_blog)

graph.add_edge(START, "create_outline")
graph.add_edge("create_outline", "create_blog")
graph.add_edge("create_blog", END)

workflow = graph.compile()
```

---

## 33. Execute Prompt Chaining

```python
final_state = workflow.invoke({
    "title": "Rise of AI in India"
})

print(final_state["outline"])
print(final_state["content"])
```

---

## 34. Why LangGraph is Powerful

Unlike LangChain chains:

* Intermediate outputs are accessible
* State evolves step-by-step
* Better debugging
* More control over workflows

---

## 35. Homework (Important)

Add **one more node**:

* `evaluate_blog`

**Prompt:**

```
Based on this outline and blog,
rate the blog from 1‚Äì10.
Return only an integer score.
```

Update:

* **State** ‚Üí add `score`
* **Graph** ‚Üí add new node + edge

---

## 36. Final Takeaway

* LangGraph is **overkill for simple linear workflows**
* But extremely powerful for:

  * Complex agents
  * Conditional logic
  * Multi-step reasoning
  * Agentic AI systems

---

# üìò Agentic AI using LangGraph ‚Äì Parallel Workflows (Simple Notes)

---

## 1Ô∏è‚É£ What this video is about

This video continues the **Agentic AI using LangGraph** playlist.

### Till now, we learned:

* What Agentic AI is
* Why LangGraph is used
* LangGraph core concepts
* Sequential (linear) workflows
* First practical example (from the last video)

### In this video:

* üëâ We learn **Parallel Workflows using LangGraph**
* üëâ Two examples are covered:

  * **Non-LLM parallel workflow** (Cricket stats)
  * **LLM-based parallel workflow** (UPSC Essay Evaluation)

---

## 2Ô∏è‚É£ What is a Parallel Workflow?

* Multiple nodes run **at the same time**
* Nodes **do not depend** on each other
* All nodes use the **same input state**
* Outputs from nodes are **merged later**

üìå **Example**:

* Strike Rate
* Boundary %
* Balls per Boundary

All these can be calculated independently, so they can run in parallel.

---

## 3Ô∏è‚É£ Example 1 ‚Äì Cricket Batsman Parallel Workflow (Non-LLM)

### üéØ Goal

Given batsman data:

* Runs
* Balls
* Fours
* Sixes

Calculate **in parallel**:

* Strike Rate
* Boundary Percentage
* Balls per Boundary

Then generate a **summary**.

---

## 4Ô∏è‚É£ Workflow Structure

```
START
 ‚îú‚îÄ‚îÄ Calculate Strike Rate
 ‚îú‚îÄ‚îÄ Calculate Boundary %
 ‚îú‚îÄ‚îÄ Calculate Balls per Boundary
      ‚Üì
    Summary
      ‚Üì
     END
```

---

## 5Ô∏è‚É£ Define State (Very Important)

The state holds **all data used in the workflow**.

```python
from typing import TypedDict

class BatsmanState(TypedDict):
    runs: int
    balls: int
    fours: int
    sixes: int
    strike_rate: float
    boundary_percent: float
    balls_per_boundary: float
    summary: str
```

---

## 6Ô∏è‚É£ Create Nodes (Functions)

### 6.1 Calculate Strike Rate

```python
def calculate_strike_rate(state: BatsmanState):
    sr = (state["runs"] / state["balls"]) * 100
    return {"strike_rate": sr}  # partial update
```

---

### 6.2 Calculate Balls per Boundary

```python
def calculate_balls_per_boundary(state: BatsmanState):
    bpb = state["balls"] / (state["fours"] + state["sixes"])
    return {"balls_per_boundary": bpb}
```

---

### 6.3 Calculate Boundary Percentage

```python
def calculate_boundary_percent(state: BatsmanState):
    boundary_runs = (state["fours"] * 4) + (state["sixes"] * 6)
    percent = (boundary_runs / state["runs"]) * 100
    return {"boundary_percent": percent}
```

---

### 6.4 Summary Node

```python
def summary_node(state: BatsmanState):
    summary = (
        f"Strike Rate: {state['strike_rate']}\n"
        f"Balls per Boundary: {state['balls_per_boundary']}\n"
        f"Boundary %: {state['boundary_percent']}"
    )
    return {"summary": summary}
```

---

## 7Ô∏è‚É£ Build the Graph

```python
from langgraph.graph import StateGraph, START, END

graph = StateGraph(BatsmanState)

graph.add_node("strike_rate", calculate_strike_rate)
graph.add_node("bpb", calculate_balls_per_boundary)
graph.add_node("boundary_percent", calculate_boundary_percent)
graph.add_node("summary", summary_node)
```

---

## 8Ô∏è‚É£ Define Edges (Parallel Execution)

```python
graph.add_edge(START, "strike_rate")
graph.add_edge(START, "bpb")
graph.add_edge(START, "boundary_percent")

graph.add_edge("strike_rate", "summary")
graph.add_edge("bpb", "summary")
graph.add_edge("boundary_percent", "summary")

graph.add_edge("summary", END)
```

---

## 9Ô∏è‚É£ Compile & Run Workflow

```python
workflow = graph.compile()

initial_state = {
    "runs": 100,
    "balls": 50,
    "fours": 6,
    "sixes": 4
}

result = workflow.invoke(initial_state)
print(result["summary"])
```

---

## üîü Important Concept ‚Äì Partial State Updates ‚ö†Ô∏è

### ‚ùå Wrong (causes error in parallel workflows)

```python
return state
```

### ‚úÖ Correct

```python
return {"strike_rate": sr}
```

üìå **Reason**:

* Parallel nodes must **not overwrite shared state**
* Only return the **field you update**
* This avoids conflicts

üëâ **Best Practice**:
Always use **partial state updates**, even in sequential workflows.

---

## 1Ô∏è‚É£1Ô∏è‚É£ Example 2 ‚Äì UPSC Essay Evaluation (LLM-Based Parallel Workflow)

### üéØ Goal

Evaluate an essay on:

* Language Quality
* Depth of Analysis
* Clarity of Thought

Each evaluation:

* Runs in parallel
* Uses an LLM
* Returns:

  * Text feedback
  * Score (0‚Äì10)

Final node:

* Merges feedback
* Calculates average score

---

## 1Ô∏è‚É£2Ô∏è‚É£ Why Structured Output is Needed

### ‚ùå Problem

* LLM may return "seven" instead of `7`
* Difficult to calculate average score

### ‚úÖ Solution

* Use **Structured Output**
* Enforce schema using **Pydantic**

---

## 1Ô∏è‚É£3Ô∏è‚É£ Define Evaluation Schema

```python
from pydantic import BaseModel, Field

class EvaluationSchema(BaseModel):
    feedback: str = Field(description="Detailed feedback")
    score: int = Field(ge=0, le=10, description="Score out of 10")
```

---

## 1Ô∏è‚É£4Ô∏è‚É£ Create Structured LLM

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
structured_llm = llm.with_structured_output(EvaluationSchema)
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ UPSC Workflow State

```python
from typing import List
from typing_extensions import Annotated
import operator

class UPSCState(TypedDict):
    essay: str
    language_feedback: str
    analysis_feedback: str
    clarity_feedback: str
    overall_feedback: str
    individual_scores: Annotated[List[int], operator.add]
    average_score: float
```

üìå `operator.add` is a **reducer function** used to merge scores from parallel nodes.

---

## 1Ô∏è‚É£6Ô∏è‚É£ Evaluation Nodes (Parallel)

### Language Evaluation

```python
def evaluate_language(state: UPSCState):
    result = structured_llm.invoke(
        f"Evaluate language quality:\n{state['essay']}"
    )
    return {
        "language_feedback": result.feedback,
        "individual_scores": [result.score]
    }
```

> Analysis and Clarity nodes are similar (only prompt and key name change).

---

## 1Ô∏è‚É£7Ô∏è‚É£ Final Evaluation Node

```python
def final_evaluation(state: UPSCState):
    avg = sum(state["individual_scores"]) / len(state["individual_scores"])

    summary_prompt = f"""
    Language Feedback: {state['language_feedback']}
    Analysis Feedback: {state['analysis_feedback']}
    Clarity Feedback: {state['clarity_feedback']}
    """

    summary = llm.invoke(summary_prompt).content

    return {
        "overall_feedback": summary,
        "average_score": avg
    }
```

---

## 1Ô∏è‚É£8Ô∏è‚É£ Key Concepts Learned üéØ

### ‚úÖ Parallel Workflows

* Nodes run simultaneously
* Use partial state updates

### ‚úÖ Structured Output

* Reliable LLM responses
* Schema-based validation

### ‚úÖ Reducer Functions

* Merge parallel outputs
* Example: `operator.add`

### ‚úÖ LangChain + LangGraph Together

* LangChain ‚Üí LLM logic
* LangGraph ‚Üí workflow orchestration

---

## 1Ô∏è‚É£9Ô∏è‚É£ Final Takeaway

Once you understand **nodes, edges, partial state updates, structured output, and reducers**, you can build **very powerful Agentic AI systems**.

This foundation will help you:

* Build real-world AI agents
* Create scalable workflows
* Avoid common parallel execution bugs

---

üöÄ **You are now ready to build parallel, production-grade Agentic AI workflows using LangGraph.**

# üìò LangGraph Conditional Workflows ‚Äì Simple English Notes

*(Agentic AI using LangGraph ‚Äì Video 7)*

---

## 1Ô∏è‚É£ What we learned before this video

In the LangGraph playlist, we have already learned **two types of workflows**:

### ‚úÖ 1. Sequential Workflow

* Tasks run **one after another**
* Linear execution

**Example:**

```
Task1 ‚Üí Task2 ‚Üí Task3 ‚Üí Task4
```

---

### ‚úÖ 2. Parallel Workflow

* Multiple tasks run **at the same time**
* Branching happens, but **all branches execute**

**Example:**

```
        ‚Üí Task2 ‚Üí
Task1                 ‚Üí Task4
        ‚Üí Task3 ‚Üí
```

---

## 2Ô∏è‚É£ What is a Conditional Workflow? (NEW TOPIC)

### üîπ Definition

A **Conditional Workflow** is a workflow where:

* Multiple branches exist
* **Only ONE branch executes**
* The branch is chosen based on a **condition**

üëâ It is exactly like **if‚Äìelse** in programming.

---

### üîπ Difference between Parallel & Conditional Workflow

| Parallel Workflow      | Conditional Workflow         |
| ---------------------- | ---------------------------- |
| All branches run       | Only one branch runs         |
| No condition           | Condition decides path       |
| Tasks execute together | Tasks are mutually exclusive |

---

### üîπ Example Flow

```
Task1
  |
  |-- if condition A ‚Üí Task2 ‚Üí Task4
  |
  |-- if condition B ‚Üí Task3 ‚Üí Task4
```

üö´ **Task2 and Task3 never run together**

---

### ‚ö†Ô∏è Why Conditional Workflows are IMPORTANT

* Almost every **real-world agentic AI system needs conditions**
* Just like `if-else` is critical in programming,
  **conditional branching is critical in LangGraph**

---

## 3Ô∏è‚É£ How we approach this topic

We build **2 workflows**:

### 1Ô∏è‚É£ Non-LLM workflow

* Quadratic Equation Solver
* Focus: **Concept clarity**

### 2Ô∏è‚É£ LLM-based workflow

* Customer Review Handling
* Focus: **Real-world AI system**

---

# üßÆ WORKFLOW 1: Quadratic Equation (Non-LLM)

---

## 4Ô∏è‚É£ Quadratic Equation Refresher

### General Form

```
ax¬≤ + bx + c = 0
```

### Discriminant

```
D = b¬≤ - 4ac
```

### Conditions

| Discriminant (D) | Roots             |
| ---------------- | ----------------- |
| D > 0            | Two real roots    |
| D = 0            | One repeated root |
| D < 0            | No real roots     |

---

## 5Ô∏è‚É£ Workflow Design

### Steps

1. Take input `a, b, c`
2. Display equation
3. Calculate discriminant
4. Conditionally choose:

   * Real roots
   * Repeated root
   * No real roots
5. End workflow

---

## 6Ô∏è‚É£ State Definition

```python
from typing import TypedDict

class QuadState(TypedDict):
    a: float
    b: float
    c: float
    equation: str
    discriminant: float
    result: str
```

---

## 7Ô∏è‚É£ Create Graph

```python
from langgraph.graph import StateGraph

graph = StateGraph(QuadState)
```

---

## 8Ô∏è‚É£ Node 1: Show Equation

```python
def show_equation(state: QuadState):
    eq = f"{state['a']}x¬≤ + {state['b']}x + {state['c']}"
    return {"equation": eq}
```

---

## 9Ô∏è‚É£ Node 2: Calculate Discriminant

```python
def calculate_discriminant(state: QuadState):
    d = state["b"]**2 - 4 * state["a"] * state["c"]
    return {"discriminant": d}
```

---

## üîü Connect Initial Nodes

```python
graph.add_node("show_equation", show_equation)
graph.add_node("calculate_discriminant", calculate_discriminant)

graph.add_edge("START", "show_equation")
graph.add_edge("show_equation", "calculate_discriminant")
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Root Calculation Nodes

### ‚úîÔ∏è Real Roots

```python
import math

def real_roots(state: QuadState):
    d = state["discriminant"]
    a = state["a"]
    b = state["b"]
    r1 = (-b + math.sqrt(d)) / (2 * a)
    r2 = (-b - math.sqrt(d)) / (2 * a)
    return {"result": f"Roots are {r1} and {r2}"}
```

---

### ‚úîÔ∏è Repeated Root

```python
def repeated_root(state: QuadState):
    a = state["a"]
    b = state["b"]
    r = -b / (2 * a)
    return {"result": f"Repeated root is {r}"}
```

---

### ‚ùå No Real Roots

```python
def no_real_roots(state: QuadState):
    return {"result": "No real roots"}
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ Condition Function (MOST IMPORTANT)

```python
def check_condition(state: QuadState):
    d = state["discriminant"]
    if d > 0:
        return "real_roots"
    elif d == 0:
        return "repeated_root"
    else:
        return "no_real_roots"
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ Conditional Edges

```python
graph.add_conditional_edges(
    "calculate_discriminant",
    check_condition
)
```

---

## 1Ô∏è‚É£4Ô∏è‚É£ End Connections

```python
graph.add_edge("real_roots", "END")
graph.add_edge("repeated_root", "END")
graph.add_edge("no_real_roots", "END")

workflow = graph.compile()
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ Run Workflow

```python
workflow.invoke({"a": 4, "b": -5, "c": -4})
```

---

### üéØ Key Learning (Workflow 1)

Conditional routing is done using:

* A **condition function**
* `add_conditional_edges()`

---

# ü§ñ WORKFLOW 2: LLM-Based Review Handling

---

## 16Ô∏è‚É£ Problem Statement

We receive a **customer review** and must:

* Detect sentiment
* Respond differently for:

  * Positive review
  * Negative review

---

## 17Ô∏è‚É£ Workflow Design

```
Review
  ‚Üì
Find Sentiment (LLM)
  ‚Üì
IF Positive ‚Üí Positive Response
IF Negative ‚Üí Diagnosis ‚Üí Negative Response
```

---

## 18Ô∏è‚É£ Structured Output: Sentiment Schema

```python
from pydantic import BaseModel
from typing import Literal

class SentimentSchema(BaseModel):
    sentiment: Literal["positive", "negative"]
```

---

## 19Ô∏è‚É£ LLM Setup

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")

structured_model = model.with_structured_output(SentimentSchema)
```

---

## 20Ô∏è‚É£ Review State

```python
from typing import TypedDict

class ReviewState(TypedDict):
    review: str
    sentiment: Literal["positive", "negative"]
    diagnosis: dict
    response: str
```

---

## 21Ô∏è‚É£ Node: Find Sentiment

```python
def find_sentiment(state: ReviewState):
    prompt = f"Find sentiment of this review:\n{state['review']}"
    result = structured_model.invoke(prompt)
    return {"sentiment": result.sentiment}
```

---

## 22Ô∏è‚É£ Condition Function

```python
def check_sentiment(state: ReviewState):
    if state["sentiment"] == "positive":
        return "positive_response"
    else:
        return "run_diagnosis"
```

---

## 23Ô∏è‚É£ Positive Response Node

```python
def positive_response(state: ReviewState):
    prompt = f"Write a warm thank you reply for:\n{state['review']}"
    res = model.invoke(prompt)
    return {"response": res.content}
```

---

## 24Ô∏è‚É£ Diagnosis Schema

```python
class DiagnosisSchema(BaseModel):
    issue_type: Literal["UI", "Performance", "Bug", "Support", "Other"]
    tone: Literal["Calm", "Frustrated", "Angry"]
    urgency: Literal["Low", "Medium", "High"]
```

---

## 25Ô∏è‚É£ Diagnosis Node

```python
diagnosis_model = model.with_structured_output(DiagnosisSchema)

def run_diagnosis(state: ReviewState):
    prompt = f"Diagnose this review:\n{state['review']}"
    result = diagnosis_model.invoke(prompt)
    return {"diagnosis": result.model_dump()}
```

---

## 26Ô∏è‚É£ Negative Response Node

```python
def negative_response(state: ReviewState):
    d = state["diagnosis"]
    prompt = f"""
User has issue: {d['issue_type']}
Tone: {d['tone']}
Urgency: {d['urgency']}
Write empathetic response.
"""
    res = model.invoke(prompt)
    return {"response": res.content}
```

---

## 27Ô∏è‚É£ Final Graph Connections

```python
graph.add_conditional_edges("find_sentiment", check_sentiment)

graph.add_edge("positive_response", "END")
graph.add_edge("run_diagnosis", "negative_response")
graph.add_edge("negative_response", "END")
```

---

## üéØ Final Takeaways

* Conditional workflows = **if‚Äìelse logic for agents**
* Core steps:

  1. Write condition function
  2. Return next node name
  3. Use `add_conditional_edges()`

üëâ This concept is used **almost everywhere in advanced Agentic AI systems**
# üìò LangGraph Iterative (Looping) Workflow ‚Äì Simple English Notes

This README explains **Iterative (Looping) Workflows in LangGraph** using a real-world example. It covers concepts, design, and complete runnable code.

---

## 1Ô∏è‚É£ What We Learned Before

In the **Agentic AI using LangGraph** playlist, we already learned three workflow types:

### ‚úÖ Sequential Workflow

* Tasks run one after another
* Linear execution

```
Task1 ‚Üí Task2 ‚Üí Task3
```

---

### ‚úÖ Parallel Workflow

* Multiple tasks run at the same time
* All branches execute

```
      ‚Üí Task2 ‚Üí
Task1              ‚Üí Task4
      ‚Üí Task3 ‚Üí
```

---

### ‚úÖ Conditional Workflow

* Multiple branches exist
* Only **one branch runs**
* Decision based on condition (if‚Äìelse)

---

## 2Ô∏è‚É£ Iterative (Looping) Workflow

### üîπ What is an Iterative Workflow?

An **Iterative (Looping) Workflow** is a workflow where:

* Tasks run again and again
* Output is gradually improved
* Loop stops when a condition is satisfied

üëâ Just like a **while loop** in programming.

### üîπ Why It Is Important

* AI output is rarely perfect on first try
* Needed for:

  * Generate
  * Evaluate
  * Improve

Used in:

* Content generation
* Code refinement
* Planning agents
* Self‚Äëimproving AI systems

---

## 3Ô∏è‚É£ Real-Life Use Case: Content Automation

### Problem

* You create YouTube videos
* No time to post on:

  * Twitter (X)
  * LinkedIn
  * Instagram

### Solution

Build an automated workflow that:

1. Takes a topic
2. Generates a tweet
3. Evaluates quality
4. Improves it until approved

---

## 4Ô∏è‚É£ High-Level Workflow Design

### Components

| Component     | Role                   |
| ------------- | ---------------------- |
| Generator LLM | Creates tweet          |
| Evaluator LLM | Judges quality         |
| Optimizer LLM | Improves tweet         |
| Loop          | Repeats until approved |

### üîÅ Flow

```
Topic
  ‚Üì
Generate Tweet
  ‚Üì
Evaluate Tweet
  ‚îú‚îÄ‚îÄ Approved ‚Üí END
  ‚îî‚îÄ‚îÄ Needs Improvement
          ‚Üì
     Optimize Tweet
          ‚Üì
       Evaluate Again
```

---

## 5Ô∏è‚É£ Defining Workflow State

In LangGraph, **state is mandatory**.

```python
from typing import TypedDict, Literal
from typing_extensions import Annotated
import operator

class TweetState(TypedDict):
    topic: str
    tweet: str
    evaluation: Literal["approved", "needs_improvement"]
    feedback: str
    iteration: int
    max_iteration: int

    tweet_history: Annotated[list[str], operator.add]
    feedback_history: Annotated[list[str], operator.add]
```

### Why These Fields?

* `topic` ‚Üí input topic
* `tweet` ‚Üí current tweet
* `evaluation` ‚Üí decision
* `feedback` ‚Üí evaluator comments
* `iteration` ‚Üí loop counter
* `max_iteration` ‚Üí safety stop
* `tweet_history` ‚Üí all tweets
* `feedback_history` ‚Üí all feedback

---

## 6Ô∏è‚É£ Creating LLMs

```python
from langchain_openai import ChatOpenAI

generator_llm = ChatOpenAI(model="gpt-4o")
evaluator_llm = ChatOpenAI(model="gpt-4o-mini")
optimizer_llm = ChatOpenAI(model="gpt-4o")
```

üëâ In real projects, select the best model per task.

---

## 7Ô∏è‚É£ Node 1: Generate Tweet

```python
from langchain.schema import SystemMessage, HumanMessage

def generate_tweet(state: TweetState):
    messages = [
        SystemMessage(content="You are a funny and clever Twitter influencer."),
        HumanMessage(content=f"""
Write a short, original, hilarious tweet on the topic: {state['topic']}

Rules:
- No Q&A format
- Max 280 characters
- Use humor, sarcasm, irony
- Simple English
""")
    ]

    response = generator_llm.invoke(messages)

    return {
        "tweet": response.content,
        "tweet_history": [response.content]
    }
```

---

## 8Ô∏è‚É£ Node 2: Evaluate Tweet (Structured Output)

### Evaluation Schema

```python
from pydantic import BaseModel
from typing import Literal

class TweetEvaluation(BaseModel):
    evaluation: Literal["approved", "needs_improvement"]
    feedback: str
```

### Evaluator Function

```python
structured_evaluator = evaluator_llm.with_structured_output(TweetEvaluation)

def evaluate_tweet(state: TweetState):
    messages = [
        SystemMessage(content="You are a ruthless Twitter critic."),
        HumanMessage(content=f"""
Evaluate the tweet below:

Tweet:
{state['tweet']}

Return:
- evaluation (approved / needs_improvement)
- feedback
""")
    ]

    result = structured_evaluator.invoke(messages)

    return {
        "evaluation": result.evaluation,
        "feedback": result.feedback,
        "feedback_history": [result.feedback]
    }
```

---

## 9Ô∏è‚É£ Node 3: Optimize Tweet

```python
def optimize_tweet(state: TweetState):
    messages = [
        SystemMessage(content="You improve tweets using feedback."),
        HumanMessage(content=f"""
Improve this tweet based on feedback:

Tweet: {state['tweet']}
Feedback: {state['feedback']}

Rules:
- Under 280 characters
- No Q&A
""")
    ]

    response = optimizer_llm.invoke(messages)

    return {
        "tweet": response.content,
        "iteration": state["iteration"] + 1,
        "tweet_history": [response.content]
    }
```

---

## üîü Routing Logic (Loop Decision)

```python
def route_evaluation(state: TweetState):
    if (
        state["evaluation"] == "approved"
        or state["iteration"] >= state["max_iteration"]
    ):
        return "approved"
    else:
        return "needs_improvement"
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Building the LangGraph

```python
from langgraph.graph import StateGraph, END

graph = StateGraph(TweetState)

graph.add_node("generate", generate_tweet)
graph.add_node("evaluate", evaluate_tweet)
graph.add_node("optimize", optimize_tweet)

graph.add_edge("start", "generate")
graph.add_edge("generate", "evaluate")

graph.add_conditional_edges(
    "evaluate",
    route_evaluation,
    {
        "approved": END,
        "needs_improvement": "optimize"
    }
)

graph.add_edge("optimize", "evaluate")

workflow = graph.compile()
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ Running the Workflow

```python
initial_state = {
    "topic": "Indian Railways",
    "iteration": 1,
    "max_iteration": 5,
    "tweet_history": [],
    "feedback_history": []
}

result = workflow.invoke(initial_state)
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ Output Analysis

* Tweet may be approved in first iteration
* Or after multiple improvements
* `tweet_history` shows evolution
* `feedback_history` shows reasoning

---

## 1Ô∏è‚É£4Ô∏è‚É£ Key Takeaways ‚≠ê

* Iterative workflows enable **self‚Äëimproving AI**
* LangGraph loops are created using:

  * Conditional edges
  * Normal edges
* Always use `max_iteration` to avoid infinite loops
* Used in:

  * AI writing agents
  * Code agents
  * Planning agents
  * Autonomous systems

---

üöÄ **You now know how to build looping agents in LangGraph!**

# üìò Building a Chatbot with Memory using LangGraph

*(Simple English Notes + Code)*

---

## 1Ô∏è‚É£ What we have learned so far in the LangGraph playlist

Till now, we have learned:

* LangGraph fundamentals
* Basics of Agentic AI
* Different types of workflows:

  * ‚úÖ Sequential Workflow
  * ‚úÖ Parallel Workflow
  * ‚úÖ Conditional Workflow
  * ‚úÖ Iterative / Looping Workflow

At this point, we know enough basics to start building real Agentic AI applications.

---

## 2Ô∏è‚É£ What we are building now

We will now deep dive and build a real chatbot using LangGraph.

### This chatbot will support:

* Normal chatting (LLM-based chat)
* Memory (remember past conversation)

### Later we will add:

* RAG (document-based answers)
* Tools (actions)
* UI
* LangSmith integration

### Advanced concepts we will cover:

* Memory
* Persistence
* Checkpointers
* Human-in-the-loop (HITL)
* Retry logic
* Fault tolerance

üëâ **This video = Part 1**
We build a **basic chatbot with memory support**.

---

## 3Ô∏è‚É£ Chatbot = Workflow (Important Concept)

A chatbot is basically a **workflow**.

### Our chatbot workflow:

* It is a **Sequential Workflow**
* It has **only ONE node**

### Flow:

```
START
  ‚Üì
User Message
  ‚Üì
Chat Node (LLM)
  ‚Üì
AI Response
  ‚Üì
END
```

This process repeats while the user keeps chatting.

---

## 4Ô∏è‚É£ What is the STATE of this workflow?

In LangGraph, **every workflow needs a state**.

### For a chatbot, what is the most important data?

üëâ **Conversation messages**

So our state will store:

* All messages exchanged between **user and AI**

---

## 5Ô∏è‚É£ Defining the Chat State

### Why not just `list[str]`?

Because LangChain / LangGraph works with **message objects**, not plain strings.

### Types of messages:

* `HumanMessage`
* `AIMessage`
* `SystemMessage`
* `ToolMessage`

All of them inherit from **BaseMessage**.

### ‚úÖ Chat State Definition

```python
from typing import Annotated, TypedDict
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage

class ChatState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
```

### Why `add_messages` reducer?

* Default state behavior **replaces old values**
* We want to **append messages**, not replace them
* `add_messages` is optimized for message lists

---

## 6Ô∏è‚É£ Creating the Graph

```python
from langgraph.graph import StateGraph, START, END

graph = StateGraph(ChatState)
```

---

## 7Ô∏è‚É£ Creating the Chat Node

This node:

* Takes messages from state
* Sends them to the LLM
* Stores the AI response back in state

### Chat Node Function

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

def chat_node(state: ChatState):
    messages = state["messages"]
    response = llm.invoke(messages)

    return {
        "messages": [response]
    }
```

---

## 8Ô∏è‚É£ Adding Node & Edges

```python
graph.add_node("chat_node", chat_node)

graph.add_edge(START, "chat_node")
graph.add_edge("chat_node", END)
```

---

## 9Ô∏è‚É£ Compile the Graph

```python
chatbot = graph.compile()
```

### Workflow structure:

```
START ‚Üí chat_node ‚Üí END
```

---

## üîü Testing the chatbot (Single message)

```python
from langchain_core.messages import HumanMessage

initial_state = {
    "messages": [HumanMessage(content="What is the capital of India?")]
}

result = chatbot.invoke(initial_state)

print(result["messages"][-1].content)
```

‚úÖ **Output:**

```
The capital of India is New Delhi.
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Problem: This is NOT a real chatbot yet

‚ùå It answers only one question
‚ùå Conversation does not continue

A real chatbot should:

* Keep chatting
* Remember previous messages

---

## 1Ô∏è‚É£2Ô∏è‚É£ Adding a Chat Loop (Console Chatbot)

```python
while True:
    user_input = input("You: ")

    if user_input.strip().lower() in ["exit", "quit", "bye"]:
        print("Chat ended.")
        break

    response = chatbot.invoke({
        "messages": [HumanMessage(content=user_input)]
    })

    ai_reply = response["messages"][-1].content
    print("AI:", ai_reply)
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ BIG PROBLEM: Chatbot forgets everything üòµ

### Example:

```
User: Hi, my name is Nitesh
AI: Hello Nitesh!

User: What is my name?
AI: Sorry, I don't know.
```

### Why this happens?

* Each `invoke()` call starts fresh
* State is **not persisted**
* Previous conversation is lost

---

## 1Ô∏è‚É£4Ô∏è‚É£ Why state is getting erased?

* Each loop iteration calls `chatbot.invoke()`
* Workflow runs ‚Üí reaches `END`
* State is discarded
* Next call starts from scratch

---

## 1Ô∏è‚É£5Ô∏è‚É£ Solution: Persistence (Memory)

### What is Persistence?

Persistence means:

* Do **NOT** delete state after workflow ends
* Store it somewhere:

  * RAM (memory)
  * Database (production)

üëâ For now, we use **RAM-based memory**.

---

## 1Ô∏è‚É£6Ô∏è‚É£ Adding Memory using Checkpointer

### Import `MemorySaver`

```python
from langgraph.checkpoint.memory import MemorySaver
```

### Create a Checkpointer

```python
checkpointer = MemorySaver()
```

### Compile graph with persistence

```python
chatbot = graph.compile(checkpointer=checkpointer)
```

---

## 1Ô∏è‚É£7Ô∏è‚É£ Thread ID (VERY IMPORTANT)

### What is a thread?

* One conversation = one thread
* Different users = different threads

```python
thread_id = "nitesh-chat"
```

---

## 1Ô∏è‚É£8Ô∏è‚É£ Passing config while invoking

```python
config = {
    "configurable": {
        "thread_id": thread_id
    }
}
```

### Updated invoke call

```python
response = chatbot.invoke(
    {
        "messages": [HumanMessage(content=user_input)]
    },
    config=config
)
```

---

## 1Ô∏è‚É£9Ô∏è‚É£ Now chatbot REMEMBERS üéâ

### Example:

```
User: Hi my name is Nitesh
AI: Hello Nitesh!

User: What is my name?
AI: Your name is Nitesh.
```

### Math example:

```
User: Add 10 to 100
AI: 110

User: Multiply the result by 2
AI: 220
```

---

## 2Ô∏è‚É£0Ô∏è‚É£ Why memory works now (Behind the scenes)

* After each workflow execution:

  * State is saved in RAM
* Next `invoke()`:

  * Previous state is fetched
  * New messages are appended

### Thanks to:

* `add_messages` reducer
* `MemorySaver` checkpointer
* `thread_id`

---

## 2Ô∏è‚É£1Ô∏è‚É£ Important Limitation of RAM Memory

‚ùå If program restarts ‚Üí memory is lost

### Production systems use:

* Databases (Postgres, Redis, etc.)

So chatbot remembers conversations even after restart.

---

## 2Ô∏è‚É£2Ô∏è‚É£ Summary

### What we built:

* ‚úÖ LangGraph-based chatbot
* ‚úÖ Message-based state
* ‚úÖ Reducer-based history handling
* ‚úÖ Loop-based chatting
* ‚úÖ Memory using persistence
* ‚úÖ Thread-based conversations

---

üéØ **Next Steps:**

* Deep dive into **Persistence**
* Understanding **Threads & Checkpointers**
* Adding **RAG, Tools, UI, and HITL**

---

üöÄ You are now building **industry-grade Agentic AI systems** with LangGraph!

# üìò LangGraph Persistence ‚Äì Simple English Notes (with Code)

---

## 1Ô∏è‚É£ What is Persistence in LangGraph?

**Persistence** means:

üëâ Ability to **save and restore the state of a workflow over time**

### In simple words:

* Normally, when a LangGraph workflow finishes ‚Üí **state is lost**
* With persistence ‚Üí **state is saved**

Later you can:

* Resume workflow
* Recover after crash
* Resume chat
* Debug past executions

üìå **Persistence = Memory for workflows**

---

## 2Ô∏è‚É£ Two Core Concepts You Already Know

### üîπ 1. Graph

* Workflow is a **graph**
* Nodes = tasks
* Edges = execution order

Example:

```
START ‚Üí Task1 ‚Üí Task2 ‚Üí END
```

---

### üîπ 2. State

* **State = dictionary**
* Stores important data during workflow execution

Example:

```python
state = {
    "messages": [],
    "topic": "pizza",
    "joke": ""
}
```

‚úî Every node can:

* Read state
* Update state

---

## 3Ô∏è‚É£ Default LangGraph Behavior (Without Persistence)

* Workflow executes
* State keeps changing
* Workflow ends
* ‚ùå State is erased from memory (RAM)

üëâ You **cannot access old state later**

---

## 4Ô∏è‚É£ What Persistence Changes

With persistence:

* State is saved externally
* State can be restored later
* Works across crashes, restarts, sessions

üìå This enables:

* Fault tolerance
* Chat history
* Resume workflows
* Debugging (time travel)

---

## 5Ô∏è‚É£ Speciality of Persistence ‚≠ê

‚ùó **Very Important Point**

Persistence does **NOT** store only final state
It stores **ALL intermediate states**

### Example workflow:

| Step   | State Value |
| ------ | ----------- |
| Start  | name = "A"  |
| Node 1 | name = "B"  |
| Node 2 | name = "C"  |
| End    | name = "C"  |

‚úî All values **A ‚Üí B ‚Üí C** are stored

---

## 6Ô∏è‚É£ Why This is Powerful?

### üîπ 1. Fault Tolerance

* If workflow crashes:

  * Resume from **last saved checkpoint**
  * Not from start

### üîπ 2. Chatbots (Resume Chat)

* Old conversations can be restored
* Exactly how ChatGPT resumes chats

---

## 7Ô∏è‚É£ Where is State Stored?

üëâ In some **database**

Examples:

* In-memory (RAM) ‚Üí demo only
* PostgreSQL ‚Üí production
* Redis ‚Üí production

---

## 8Ô∏è‚É£ Checkpointer (MOST IMPORTANT)

### üîπ What is a Checkpointer?

* Persistence is implemented using **Checkpointers**
* Checkpointer:

  * Splits workflow into checkpoints
  * Saves state at each checkpoint

üìå Checkpoints are created at **every superstep**

---

### üîπ What is a Superstep?

* A **superstep** = group of nodes running together
* Parallel nodes = one superstep

Each superstep ‚Üí **1 checkpoint**

---

## 9Ô∏è‚É£ Example of Checkpointer (Numbers Example)

### State

```python
numbers: List[int]
```

### Execution

| Checkpoint     | State           |
| -------------- | --------------- |
| Start          | [1]             |
| Node 1         | [1, 2]          |
| Parallel Nodes | [1, 2, 3, 4, 5] |
| End            | [1, 2, 3, 4, 5] |

‚úî All saved in database

---

## üîü Threads (VERY IMPORTANT)

### üîπ What is a Thread?

* Every workflow execution gets a **thread_id**
* Helps separate multiple executions

Example:

```
Thread 1 ‚Üí Pizza joke
Thread 2 ‚Üí Pasta joke
```

üìå Without `thread_id` ‚Üí all states get mixed

---

### üîπ Why Threads Matter?

* Resume **specific execution**
* Resume **specific chat**
* Load correct state from database

---

## 1Ô∏è‚É£1Ô∏è‚É£ Basic Persistence Code (In-Memory Demo)

### üîπ Imports

```python
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver
```

---

### üîπ Define State

```python
from typing import TypedDict

class JokeState(TypedDict):
    topic: str
    joke: str
    explanation: str
```

---

### üîπ Node Functions

```python
def generate_joke(state: JokeState):
    joke = f"Why did the {state['topic']} go to the doctor? Because it felt cheesy!"
    return {"joke": joke}


def generate_explanation(state: JokeState):
    explanation = f"It is funny because {state['topic']} is related to cheese."
    return {"explanation": explanation}
```

---

### üîπ Build Graph

```python
graph = StateGraph(JokeState)

graph.add_node("generate_joke", generate_joke)
graph.add_node("generate_explanation", generate_explanation)

graph.set_entry_point("generate_joke")
graph.add_edge("generate_joke", "generate_explanation")
graph.set_finish_point("generate_explanation")
```

---

### üîπ Enable Persistence

```python
checkpointer = MemorySaver()

workflow = graph.compile(checkpointer=checkpointer)
```

---

### üîπ Run Workflow (with Thread ID)

```python
config = {"configurable": {"thread_id": "1"}}

workflow.invoke(
    {"topic": "pizza"},
    config=config
)
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ Get Final State

```python
workflow.get_state(config)
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ Get State History (All Checkpoints)

```python
workflow.get_state_history(config)
```

‚úî Shows:

* Initial state
* Before each node
* After each node

---

## 1Ô∏è‚É£4Ô∏è‚É£ Resume Workflow After Crash

```python
workflow.invoke(
    None,  # resume
    config=config
)
```

‚úî Workflow resumes from last checkpoint

---

## 1Ô∏è‚É£5Ô∏è‚É£ Time Travel ‚è≥

### üîπ What is Time Travel?

* Go back to any checkpoint
* Replay workflow from there
* Debug complex workflows

---

### üîπ Get Specific Checkpoint State

```python
workflow.get_state(
    config=config,
    checkpoint_id="checkpoint-id-here"
)
```

---

### üîπ Replay from Checkpoint

```python
workflow.invoke(
    None,
    config={
        "configurable": {
            "thread_id": "1",
            "checkpoint_id": "checkpoint-id-here"
        }
    }
)
```

---

## 1Ô∏è‚É£6Ô∏è‚É£ Update Past State (Advanced)

```python
workflow.update_state(
    config=config,
    checkpoint_id="checkpoint-id-here",
    values={"topic": "samosa"}
)
```

‚úî Creates a **new branch of execution**

---

## 1Ô∏è‚É£7Ô∏è‚É£ Benefits of Persistence (FINAL)

‚úÖ **1. Short-Term Memory**

* Resume chat conversations

‚úÖ **2. Fault Tolerance**

* Resume after crashes

‚úÖ **3. Human-in-the-Loop (HITL)**

* Pause workflow
* Wait for human input
* Resume later

‚úÖ **4. Time Travel**

* Replay execution
* Debug workflows

---

## üéØ Final Summary

* Persistence = backbone of LangGraph
* Implemented using **Checkpointers**
* Uses **Threads** to separate executions

### Enables:

* Chat memory
* Crash recovery
* HITL
* Debugging

üìå **Without persistence ‚Üí LangGraph is incomplete**

# üìò Building a Chatbot UI using LangGraph + Streamlit

*(Simple English Notes with Code)*

---

## 1Ô∏è‚É£ What problem are we solving?

In the previous video, we built a LangGraph chatbot with:

* Working logic ‚úÖ
* Short-term memory ‚úÖ

‚ùå **But there was one big problem**
The chatbot had **no UI (User Interface)**.

We were chatting with it only inside:

* Jupyter Notebook
* Terminal

üëâ This is **not user-friendly**.

---

## 2Ô∏è‚É£ Goal of this video

We will:

* ‚úÖ Give our LangGraph chatbot a **proper web UI**
* ‚úÖ Build a **chat-style interface (like ChatGPT)**
* ‚úÖ Use **Streamlit** to create the frontend
* ‚úÖ Connect **Streamlit (Frontend)** with **LangGraph (Backend)**

---

## 3Ô∏è‚É£ Final Output (What we will build)

* Clean chat UI
* User and AI messages appear differently
* Automatic scrolling
* Chatbot remembers previous messages
* Supports:

  * General questions
  * Follow-up questions
  * Memory-based questions ("What is my name?")

---

## 4Ô∏è‚É£ Tech Stack Used

| Layer         | Technology              |
| ------------- | ----------------------- |
| Backend       | LangGraph               |
| LLM           | LangChain-supported LLM |
| Frontend      | Streamlit               |
| Memory        | LangGraph Checkpointer  |
| State Storage | Streamlit Session State |

---

## 5Ô∏è‚É£ High-Level Architecture

```
User
  ‚Üì
Streamlit UI (Frontend)
  ‚Üì
LangGraph Chatbot (Backend)
  ‚Üì
LLM
  ‚Üì
Response ‚Üí UI
```

---

## 6Ô∏è‚É£ Project Structure

```
project/
‚îÇ
‚îú‚îÄ‚îÄ langgraph_backend.py   # Chatbot logic (LangGraph)
‚îú‚îÄ‚îÄ streamlit_frontend.py  # UI code (Streamlit)
‚îú‚îÄ‚îÄ .env                   # API keys
‚îî‚îÄ‚îÄ venv/                  # Virtual environment
```

---

## 7Ô∏è‚É£ Backend Code (LangGraph)

This code is the **same as the previous chatbot**, no major changes.

### üîπ Key Points

* Defines state
* Creates a simple graph
* Uses **InMemory Checkpointer**
* Exposes a `chatbot` object

### üîπ Example (Simplified)

```python
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.checkpoint.memory import MemorySaver

class ChatState(dict):
    messages: list

def chat_node(state):
    # Call LLM here
    response = llm.invoke(state["messages"])
    return {"messages": state["messages"] + [response]}

builder = StateGraph(ChatState)
builder.add_node("chat", chat_node)
builder.set_entry_point("chat")
builder.add_edge("chat", END)

checkpointer = MemorySaver()
chatbot = builder.compile(checkpointer=checkpointer)
```

---

## 8Ô∏è‚É£ Why we split Backend & Frontend?

| Backend            | Frontend        |
| ------------------ | --------------- |
| LangGraph workflow | Streamlit UI    |
| LLM calls          | Input box       |
| Memory             | Message display |

üëâ **Clean separation = scalable design**

---

## 9Ô∏è‚É£ Streamlit Chat UI ‚Äì Core Components

Streamlit provides **two special chat components**:

### 1Ô∏è‚É£ `st.chat_message()`

Used to display messages

### 2Ô∏è‚É£ `st.chat_input()`

Used to take user input

---

## üîü Displaying Chat Messages

### User Message

```python
import streamlit as st

with st.chat_message("user"):
    st.text("Hi")
```

### Assistant Message

```python
with st.chat_message("assistant"):
    st.text("Hello! How can I help you?")
```

üëâ Streamlit automatically shows icons for:

* user üë§
* assistant ü§ñ

---

## 1Ô∏è‚É£1Ô∏è‚É£ Chat Input Box

```python
user_input = st.chat_input("Type here...")
```

* Appears at the bottom
* Press **Enter** to submit
* Value stored in `user_input`

---

## 1Ô∏è‚É£2Ô∏è‚É£ Problem: Messages disappear on new input

### Why?

Streamlit **reruns the entire script** every time:

* User presses Enter
* User interacts with UI

‚ùå Old messages are lost

---

## 1Ô∏è‚É£3Ô∏è‚É£ Solution: `st.session_state`

### What is Session State?

* A dictionary that **persists across reruns**
* Data stays until the page is refreshed

---

## 1Ô∏è‚É£4Ô∏è‚É£ Initialize Message History

```python
if "message_history" not in st.session_state:
    st.session_state.message_history = []
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ Message Format (Very Important)

Each message is stored as a dictionary:

```python
{
    "role": "user",        # or "assistant"
    "content": "Hello"
}
```

All messages are stored inside a list.

---

## 1Ô∏è‚É£6Ô∏è‚É£ Display Full Chat History

```python
for msg in st.session_state.message_history:
    with st.chat_message(msg["role"]):
        st.text(msg["content"])
```

---

## 1Ô∏è‚É£7Ô∏è‚É£ Add New User Message

```python
if user_input:
    st.session_state.message_history.append({
        "role": "user",
        "content": user_input
    })

    with st.chat_message("user"):
        st.text(user_input)
```

---

## 1Ô∏è‚É£8Ô∏è‚É£ Dummy Chatbot (Copy-Cat Bot)

Assistant replies with the **same message**:

```python
assistant_reply = user_input

st.session_state.message_history.append({
    "role": "assistant",
    "content": assistant_reply
})

with st.chat_message("assistant"):
    st.text(assistant_reply)
```

‚úÖ This confirms:

* UI works
* Memory works
* Session state works

---

## 1Ô∏è‚É£9Ô∏è‚É£ Connecting Streamlit to LangGraph

### Import chatbot object

```python
from langgraph_backend import chatbot
from langchain_core.messages import HumanMessage
```

---

## 2Ô∏è‚É£0Ô∏è‚É£ Calling `chatbot.invoke()`

```python
response = chatbot.invoke(
    {
        "messages": [HumanMessage(content=user_input)]
    },
    config={
        "configurable": {
            "thread_id": "thread-1"
        }
    }
)
```

‚ö†Ô∏è `thread_id` is **required** because:

* We are using a checkpointer
* Needed for memory tracking

---

## 2Ô∏è‚É£1Ô∏è‚É£ Extract AI Response

```python
ai_message = response["messages"][-1].content
```

---

## 2Ô∏è‚É£2Ô∏è‚É£ Display AI Response

```python
st.session_state.message_history.append({
    "role": "assistant",
    "content": ai_message
})

with st.chat_message("assistant"):
    st.text(ai_message)
```

---

## 2Ô∏è‚É£3Ô∏è‚É£ Final Result

üéâ You now have:

* LangGraph-powered chatbot
* Streamlit UI
* Persistent memory
* Real AI responses
* Clean frontend‚Äìbackend separation

---

## 2Ô∏è‚É£4Ô∏è‚É£ Key Takeaways

* Streamlit reruns scripts ‚Üí use `session_state`
* Chat UI = `chat_message` + `chat_input`
* LangGraph backend stays clean
* UI logic stays simple
* This design scales easily (RAG, tools, streaming)

---

üöÄ **You now have a production-ready foundation for Agentic AI chatbots.**

# üìò Streaming Responses in a LangGraph Chatbot

(Simple English Notes)

---

## 1Ô∏è‚É£ Background: What We Have Built So Far

In the **Agentic AI using LangGraph** series, we are building a chatbot step by step.

So far, we have already added:

* ‚úÖ Basic chatbot (LLM chat)
* ‚úÖ Short-term memory (chat remembers previous messages)
* ‚úÖ UI using **Streamlit**

Now, we are solving **one important UX problem**.

---

## 2Ô∏è‚É£ The Problem with Long Responses (Without Streaming)

### ‚ùå Current behavior

When we ask a long question, for example:

> "Write a 500-word blog on cricket"

What happens?

* The screen stays **blank** for 5‚Äì10 seconds
* Suddenly, the **entire response appears at once**

### ‚ùå Why this is bad?

* User has to wait silently
* Feels like the app is frozen
* Large text appearing suddenly is hard to read

üëâ This is **not how ChatGPT behaves**

---

## 3Ô∏è‚É£ What is Streaming? (Core Concept)

### ‚úÖ Definition (Simple)

**Streaming means:**

> The model sends output **token by token** as soon as it generates them
> instead of waiting for the full response.

### Two ways an LLM can respond

| Without Streaming                | With Streaming                    |
| -------------------------------- | --------------------------------- |
| Generate full answer ‚Üí send once | Generate token ‚Üí send immediately |
| Blank screen for seconds         | Text appears gradually            |
| Poor UX                          | Excellent UX                      |

üìå The typing / typewriter effect you see in ChatGPT = **Streaming**

---

## 4Ô∏è‚É£ Why Streaming is Important (Very Important)

### üîπ 1. Faster perceived response

* Even if full response takes 10 seconds
* User sees output immediately
* App feels alive, not frozen

---

### üîπ 2. Human-like conversation

* Feels like someone is typing
* Builds trust
* Keeps user engaged

---

### üîπ 3. Required for voice & multimodal apps

Example: **Alexa / Voice Assistant**

‚ùå Without streaming:

* Silence for 10 seconds
* Then sudden speech

‚úÖ With streaming:

* Device starts speaking immediately
* Feels natural

---

### üîπ 4. Much better for long outputs (Code, Blogs)

* Code appearing line-by-line is easier to understand
* Sudden full code dump is confusing

---

### üîπ 5. Saves tokens & money üí∞

* User can stop generation midway
* Fewer tokens generated
* Lower API cost (LLMs charge per token)

---

### üîπ 6. Useful for agent progress updates

Example:

> "Booking movie ticket‚Ä¶"

Streaming updates:

* Opening BookMyShow
* Selecting movie
* Selecting seats
* Making payment

üëâ User always knows what is happening

---

## 5Ô∏è‚É£ How Streaming Works in LangGraph (Key Idea)

### ‚ùó One small but powerful change

**Before:**

```python
chatbot.invoke(initial_state, config)
```

**Now:**

```python
chatbot.stream(initial_state, config, stream_mode="messages")
```

### Important difference

* `.invoke()` ‚Üí returns full result
* `.stream()` ‚Üí returns a **generator**

---

## 6Ô∏è‚É£ Python Generator (Very Important)

### What is a Generator?

A generator:

* Produces values one at a time
* Uses `yield` instead of `return`

Example:

```python
def demo():
    yield "Hello"
    yield "World"
```

üëâ Generators are perfect for **streaming tokens**

---

## 7Ô∏è‚É£ Backend: Streaming from LangGraph

### Calling `.stream()`

```python
from langchain_core.messages import HumanMessage

stream = chatbot.stream(
    {
        "messages": [HumanMessage(content=user_input)]
    },
    config={"configurable": {"thread_id": "1"}},
    stream_mode="messages"
)
```

This returns a **generator**.

Each item contains:

* `message_chunk`
* `metadata`

---

### Printing tokens one by one (Backend test)

```python
for message_chunk, metadata in stream:
    if message_chunk.content:
        print(message_chunk.content, end=" ")
```

‚úÖ Output now appears **token by token**

---

## 8Ô∏è‚É£ Important Note About Backend

üëâ **No backend changes are required permanently**

* Backend logic remains the same
* Streaming is mainly handled in **frontend (Streamlit)**

---

## 9Ô∏è‚É£ Frontend: Streaming in Streamlit

Streamlit provides a built-in function:

### ‚úÖ `st.write_stream()`

* Accepts a generator
* Automatically shows **typewriter effect**

---

## üîü Streamlit UI Code (Core Change)

### Before (Non-Streaming)

```python
response = chatbot.invoke(...)
st.chat_message("assistant").write(response)
```

---

### After (Streaming)

```python
with st.chat_message("assistant"):
    ai_message = st.write_stream(
        message_chunk.content
        for message_chunk, metadata in chatbot.stream(
            {
                "messages": [HumanMessage(content=user_input)]
            },
            config={"configurable": {"thread_id": "1"}},
            stream_mode="messages"
        )
        if message_chunk.content
    )
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Saving the Final Response in Session State

After streaming completes:

```python
st.session_state.messages.append({
    "role": "assistant",
    "content": ai_message
})
```

This ensures:

* Chat history is preserved
* Memory works correctly

---

## 1Ô∏è‚É£2Ô∏è‚É£ Common Bug (Very Important)

### ‚ùå Mistake

```python
HumanMessage(content="What is the recipe to make pasta?")
```

### ‚úÖ Correct

```python
HumanMessage(content=user_input)
```

Otherwise, the chatbot will always respond to the **same hardcoded question**.

---

## 1Ô∏è‚É£3Ô∏è‚É£ Final Result üéâ

Now your chatbot:

* ‚úÖ Streams responses token-by-token
* ‚úÖ Feels like ChatGPT
* ‚úÖ No waiting on blank screen
* ‚úÖ Better UX
* ‚úÖ Works perfectly for long answers & code

---

## 1Ô∏è‚É£4Ô∏è‚É£ Final Summary

### What we learned

* What streaming is
* Why streaming is critical for LLM apps
* How LangGraph streaming works
* How Python generators help
* How to implement streaming in Streamlit

üìå **Streaming is a small feature but gives 10x better user experience**

üìò Resume Chat Feature in LangGraph Chatbot

(Simple English Notes with Code)

1Ô∏è‚É£ What we have built so far (Recap)

In the Agentic AI using LangGraph series, we have been improving a chatbot step by step.

So far, we have added:

‚úÖ Basic chatbot (console-based)
‚úÖ Streamlit UI
‚úÖ Streaming responses (token-by-token output)

üëâ Streaming improved user experience because output appears instantly instead of waiting.

2Ô∏è‚É£ What is the goal of this video?

In this video, we add an important real-world feature:

üîÑ Resume Chat Feature

Just like ChatGPT, users can:

Start a new chat

Resume old conversations

Each conversation has its own memory

Example:

One chat remembers ‚ÄúMy name is Nitesh‚Äù

Another chat remembers ‚ÄúMy name is Rahul‚Äù

3Ô∏è‚É£ Final UI after this feature

The chatbot UI will have two sections:

üñ•Ô∏è Main Area

Chat with the AI

Messages appear with memory

üìÇ Sidebar

New Chat button

My Conversations list

Click any conversation to resume it

4Ô∏è‚É£ Important Architecture Decision
‚ùó No backend changes needed

Our chatbot is built in two parts:

Layer	Tech
Backend	LangGraph
Frontend	Streamlit

üëâ Resume Chat is purely a frontend feature
Backend already supports threads & state.

5Ô∏è‚É£ Breaking the feature into small tasks

Instead of coding everything at once, we divide it into small tasks:

Build Sidebar UI

Generate dynamic Thread IDs

Store all thread IDs

Start New Chat

Resume Old Chat

üëâ This approach makes coding easy and bug-free

6Ô∏è‚É£ Task 1: Sidebar UI
What we want in sidebar:

Title

New Chat button

My Conversations section

Code (Streamlit)
st.sidebar.title("LangGraph Chatbot")

if st.sidebar.button("New Chat"):
    pass

st.sidebar.header("My Conversations")


‚úÖ Sidebar UI is now visible

7Ô∏è‚É£ Task 2: Dynamic Thread ID generation
‚ùå Problem

Earlier we used:

thread_id = "thread1"


This fails when multiple chats exist.

‚úÖ Solution: Use UUID
Utility function
import uuid

def generate_thread_id():
    return str(uuid.uuid4())

Store thread ID in session
if "thread_id" not in st.session_state:
    st.session_state.thread_id = generate_thread_id()

Use thread ID in LangGraph config
config = {
    "configurable": {
        "thread_id": st.session_state.thread_id
    }
}

8Ô∏è‚É£ Task 3: Display current thread ID
st.sidebar.text(st.session_state.thread_id)


Now users can see which conversation is active.

9Ô∏è‚É£ Task 4: New Chat functionality
What should happen when user clicks ‚ÄúNew Chat‚Äù?

‚úî Generate new thread ID
‚úî Replace old thread ID
‚úî Clear message history

Utility function: Reset Chat
def reset_chat():
    st.session_state.thread_id = generate_thread_id()
    st.session_state.message_history = []

Button click logic
if st.sidebar.button("New Chat"):
    reset_chat()


‚úÖ New empty chat opens
‚úÖ Old messages disappear
‚ùå But old threads are lost (problem)

üî¥ Problem Found

When creating a new chat:

Old conversations disappear

Thread IDs are lost

1Ô∏è‚É£0Ô∏è‚É£ Task 5: Store all conversations (Thread List)
Create a list in session
if "chat_threads" not in st.session_state:
    st.session_state.chat_threads = []

Utility function: Add Thread
def add_thread(thread_id):
    if thread_id not in st.session_state.chat_threads:
        st.session_state.chat_threads.append(thread_id)

Add thread on page load
add_thread(st.session_state.thread_id)

Add thread inside reset_chat()
def reset_chat():
    new_id = generate_thread_id()
    st.session_state.thread_id = new_id
    st.session_state.message_history = []
    add_thread(new_id)

1Ô∏è‚É£1Ô∏è‚É£ Show all threads in Sidebar
for tid in st.session_state.chat_threads:
    st.sidebar.text(tid)


‚úÖ All conversation IDs stay visible
‚úÖ Old chats are not lost

1Ô∏è‚É£2Ô∏è‚É£ Make threads clickable (Buttons)
for tid in st.session_state.chat_threads:
    if st.sidebar.button(str(tid)):
        pass


Now threads are clickable.

1Ô∏è‚É£3Ô∏è‚É£ Resume chat: Load old messages
How do we fetch messages from LangGraph?

LangGraph stores state per thread_id.

Fetch state
state = chatbot.get_state({
    "configurable": {"thread_id": thread_id}
})
messages = state.values["messages"]

1Ô∏è‚É£4Ô∏è‚É£ Convert LangGraph messages ‚Üí UI format

LangGraph format:

HumanMessage

AIMessage

UI format:

{
  "role": "user" | "assistant",
  "content": "text"
}

Utility function: Load Conversation
from langchain_core.messages import HumanMessage

def load_conversation(thread_id):
    state = chatbot.get_state({
        "configurable": {"thread_id": thread_id}
    })
    msgs = state.values["messages"]

    temp_messages = []

    for msg in msgs:
        role = "user" if isinstance(msg, HumanMessage) else "assistant"
        temp_messages.append({
            "role": role,
            "content": msg.content
        })

    st.session_state.message_history = temp_messages
    st.session_state.thread_id = thread_id

1Ô∏è‚É£5Ô∏è‚É£ Connect Resume Logic to Sidebar Button
for tid in reversed(st.session_state.chat_threads):
    if st.sidebar.button(str(tid)):
        load_conversation(tid)


‚úÖ Clicking a thread:

Loads old messages

Restores memory

Sets correct thread ID

1Ô∏è‚É£6Ô∏è‚É£ Final Result (Demo)

‚úî Multiple conversations
‚úî Each has its own memory
‚úî Resume works perfectly
‚úî New chat doesn‚Äôt destroy old chats

Example:

Chat 1 ‚Üí ‚ÄúMy name is Nitesh‚Äù

Chat 2 ‚Üí ‚ÄúMy name is Rahul‚Äù

Switching works correctly

1Ô∏è‚É£7Ô∏è‚É£ Small UI Improvement

Show latest chat first

for tid in reversed(st.session_state.chat_threads):
    ...

üß† Homework for You

Instead of showing raw UUIDs:

üëâ Generate human-readable titles like:

‚ÄúFactorial Code‚Äù

‚ÄúAI Job Crisis Blog‚Äù

‚ÄúPython Swap Program‚Äù

(Hint: Use first user message as title)

üî• Important Limitation (Next Video Topic)

Currently:

Memory is stored in RAM

Page refresh ‚Üí all chats lost

Why?

We are using InMemory Checkpointer

Next Video:

‚úÖ Connect LangGraph to a database
‚úÖ Persistent conversations
‚úÖ Chats survive refresh

# üìò LangGraph Chatbot ‚Äì Database Persistence using SQLite Checkpointer

(Simple English Notes with Code)

---

## 1Ô∏è‚É£ What we have built so far (Recap)

In the previous videos of **Agentic AI using LangGraph**, we built a chatbot step by step.

### ‚úÖ Progress till now

* Basic LangGraph chatbot (console-based)
* UI added using Streamlit
* Streaming responses
* Thread-based conversations

üëâ Each thread represents a **separate conversation**.

üëâ Users can **switch between past chats**.

### üß† Current chatbot features

* Start a new chat
* Resume old conversations
* Ask memory-based questions like:

  * *"What is my name?"*
* Different threads remember different users

---

## 2Ô∏è‚É£ The Big Problem (Very Important)

### ‚ùå What was wrong?

We were using **InMemorySaver** for memory.

That means:

* All conversations were stored in **RAM**
* When:

  * App stops ‚ùå
  * Page refreshes ‚ùå

üëâ **All chats are lost**

### Example

* You say: *"Hi, my name is Nitesh"*
* App restarts
* Bot forgets everything üò¢

---

## 3Ô∏è‚É£ Goal of This Video

### ‚úÖ What we want to achieve

* Store conversations **permanently**
* Chats should **not disappear**
* Even after:

  * App restart
  * Browser refresh
  * Multiple days later

üëâ **Solution: Database-based persistence**

---

## 4Ô∏è‚É£ LangGraph Persistence Options

LangGraph supports different **Checkpointers**:

| Checkpointer  | Storage    | Use case                 |
| ------------- | ---------- | ------------------------ |
| InMemorySaver | RAM        | Testing only             |
| SQLiteSaver   | SQLite DB  | Learning / Prototyping ‚úÖ |
| PostgresSaver | PostgreSQL | Production               |

üëâ In this video, we use **SQLiteSaver**

---

## 5Ô∏è‚É£ Step 1: Install SQLite Checkpointer

This is **not built-in** yet.

```bash
pip install langgraph-checkpoint-sqlite
```

---

## 6Ô∏è‚É£ Backend Changes (Most Important Part)

### üîπ Replace InMemorySaver

‚ùå Old:

```python
from langgraph.checkpoint.memory import MemorySaver
checkpointer = MemorySaver()
```

‚úÖ New:

```python
from langgraph.checkpoint.sqlite import SQLiteSaver
import sqlite3
```

---

### üîπ Create SQLite Database Connection

```python
conn = sqlite3.connect(
    "chatbot.db",
    check_same_thread=False
)
```

#### ‚ùì Why `check_same_thread=False`?

* SQLite normally allows **single-thread access**
* Our chatbot uses **multiple threads**
* This disables SQLite‚Äôs thread restriction

---

### üîπ Create SQLite Checkpointer

```python
checkpointer = SQLiteSaver(conn)
```

‚úÖ That‚Äôs it!

Now LangGraph automatically:

* Stores messages
* Stores state
* Stores checkpoints

---

## 7Ô∏è‚É£ Testing Database Persistence (Backend)

### üîπ Send a message

```python
response = chatbot.invoke(
    {"messages": [("user", "Hi my name is Nitesh")]},
    config={"configurable": {"thread_id": "thread-1"}}
)

print(response)
```

### üîπ What happens?

* A file **chatbot.db** is created
* Messages are saved inside the database

---

### üîπ Restart program & ask again

```python
response = chatbot.invoke(
    {"messages": [("user", "What is my name?")]},
    config={"configurable": {"thread_id": "thread-1"}}
)
```

‚úÖ Output:

```
Your name is Nitesh
```

üëâ Memory is restored from database üéâ

---

## 8Ô∏è‚É£ Thread-wise Memory (Very Powerful Feature)

### Thread 1

```python
thread_id = "thread-1"
Hi my name is Nitesh
```

### Thread 2

```python
thread_id = "thread-2"
Hi my name is Rahul
```

### Asking the same question

```text
What is my name?
```

| Thread   | Answer |
| -------- | ------ |
| thread-1 | Nitesh |
| thread-2 | Rahul  |

üëâ Same chatbot, **different memories per thread**

---

## 9Ô∏è‚É£ Viewing SQLite Database (Optional but Recommended)

### Best way (VS Code)

1. Install **SQLite Viewer** extension
2. Open `chatbot.db`
3. View:

   * Threads
   * Messages
   * Checkpoints

You‚Äôll observe:

* Multiple checkpoints per thread
* Each execution creates **3 checkpoints**:

  * Start
  * Chat node
  * End

---

## üîü Extracting All Existing Threads (Backend)

### ‚ùì Why needed?

Frontend must:

* Show old threads
* Not start from an empty list

---

### üîπ Get all checkpoints

```python
checkpoints = checkpointer.list(None)
```

---

### üîπ Extract unique thread IDs

```python
def retrieve_all_threads():
    all_threads = set()

    for checkpoint in checkpointer.list(None):
        thread_id = checkpoint.config["configurable"]["thread_id"]
        all_threads.add(thread_id)

    return list(all_threads)
```

‚úÖ Returns:

```python
["thread-1", "thread-2"]
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Frontend Changes (Streamlit)

### ‚ùå Old behavior

```python
chat_threads = []
```

### ‚úÖ New behavior (Fetch from database)

```python
from langgraph_database_backend import chatbot, retrieve_all_threads

chat_threads = retrieve_all_threads()
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ Final Result (What We Achieved)

### ‚úÖ Features now available

* Persistent chat history
* Thread-based memory
* Database-backed storage

Chats survive:

* Refresh
* Restart
* Days later

### üßë‚Äçüíª User Experience

* Old chats automatically load
* Conversations resume exactly where left off
* Works just like **ChatGPT**

---

## 1Ô∏è‚É£3Ô∏è‚É£ Key Takeaways

* **Persistence = Game changer**
* SQLite is perfect for:

  * Learning
  * Prototyping
* Thread IDs control memory isolation
* LangGraph handles database logic internally

---

üéâ You have now built a **persistent, production-style chatbot foundation using LangGraph**!

# üìò LangGraph Chatbot ‚Äì Observability & Thread-based Tracing using LangSmith

*(Simple English Notes with Code)*

---

## 1Ô∏è‚É£ Recap: Journey So Far (Agentic AI using LangGraph)

Till now in this playlist, we have covered:

### Theory

* What is Agentic AI
* What is LangGraph
* Why LangGraph is needed

### Practical Learning

* LangGraph fundamentals
* Different workflow types
* A real chatbot project

### Chatbot Features Built So Far

‚úÖ GUI (UI for users)

‚úÖ Streaming responses (no waiting for full answer)

‚úÖ Database persistence (chats are not lost after restart)

üìå **Result**

If a user chats today, closes the app, and opens it after days ‚Üí
üëâ Old chats are still available.

---

## 2Ô∏è‚É£ What We Are Adding Today: Observability

### üîπ What is Observability?

In simple words:

> **Observability = Ability to see what is happening inside your chatbot**

For our chatbot, it means:

* Tracking user messages
* Tracking LLM responses
* Recording token usage
* Measuring latency
* Understanding system behavior internally

---

## 3Ô∏è‚É£ Tool Used for Observability: LangSmith

LangSmith is used to:

* Trace each chatbot interaction
* Debug LLM behavior
* Monitor performance
* Analyze token usage & latency

üìå **Important**

LangSmith works automatically once configured.
üëâ No code change needed in chatbot logic initially.

---

## 4Ô∏è‚É£ High-Level Concept: Tracing

Every time a user:

* Sends a message
* Gets a reply from chatbot

‚û°Ô∏è That full interaction is saved as a **TRACE**

A trace contains:

* Input
* Output
* Tokens used
* Execution time
* Node details
* Model used

---

## 5Ô∏è‚É£ Step 1: Create LangSmith Account & API Key

### üîπ Website

```
https://smith.langchain.com
```

### Steps:

1. Sign up / Login
2. Go to **Settings**
3. Open **API Keys**
4. Click **Create API Key**
5. Copy the key safely

---

## 6Ô∏è‚É£ Step 2: Environment Variable Setup (MOST IMPORTANT)

Create a `.env` file in your project folder.

### ‚úÖ Required Environment Variables

```
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=your_api_key_here
LANGSMITH_PROJECT=chatbot-project
```

üìå **Meaning**

* `LANGSMITH_TRACING=true` ‚Üí enables tracing
* `LANGSMITH_ENDPOINT` ‚Üí LangSmith backend
* `LANGSMITH_API_KEY` ‚Üí authentication
* `LANGSMITH_PROJECT` ‚Üí project name in dashboard

‚ö†Ô∏è Once this is set ‚Üí tracing starts automatically

---

## 7Ô∏è‚É£ No Code Change Needed (Initial Setup)

After adding env variables:

* Run your chatbot normally
* LangSmith automatically records traces behind the scenes

---

## 8Ô∏è‚É£ What Happens After Running Chatbot?

1. User sends a message
2. Chatbot replies normally
3. LangSmith:

   * Captures a trace
   * Stores it under your project
4. Project appears in LangSmith Dashboard

---

## 9Ô∏è‚É£ Understanding LangSmith Dashboard Structure

### üîπ Project

Top-level container

Example:

```
chatbot-project
```

### üîπ Trace

Each user turn = 1 trace

Example:

* User asks: *"Roadmap for AI Engineering"*
* Bot replies

‚û°Ô∏è That = **1 trace**

### üîπ Trace Details

Inside a trace you can see:

* Node name (Chat node)
* Model used (ChatOpenAI / Cerebras etc.)
* Input text
* Output text
* Token usage (input + output)
* Latency
* Status
* Start & end time

---

## üî¥ Problem Without Threads

If user:

* Opens multiple conversations (threads)
* Switches between them

‚ùå All traces go into the same list

‚ùå Conversations get mixed

‚ùå Hard to analyze user-wise or session-wise chats

---

## ‚úÖ Solution: Thread-based Tracing

LangSmith supports **Threads**.

Each conversation thread:

* Has its own trace group
* Keeps chat history clean & organized

---

## 1Ô∏è‚É£0Ô∏è‚É£ Requirement for Thread Logging

You must explicitly pass **one of these** while invoking the chatbot:

* `thread_id`
* `session_id`
* `conversation_id`

---

## 1Ô∏è‚É£1Ô∏è‚É£ Code Change Required (IMPORTANT)

### üîπ Old Config (Before)

```python
config = {
    "configurable": {
        "thread_id": session["thread_id"]
    }
}
```

### üîπ New Config (With Thread Metadata for LangSmith)

```python
config = {
    "configurable": {
        "thread_id": session["thread_id"]
    },
    "metadata": {
        "thread_id": session["thread_id"]
    },
    "run_name": "chat_turn"
}
```

### ‚úÖ What Changed?

| Field                | Purpose                              |
| -------------------- | ------------------------------------ |
| `metadata.thread_id` | Enables thread grouping in LangSmith |
| `run_name`           | Improves trace readability           |

---

## 1Ô∏è‚É£2Ô∏è‚É£ Why `run_name = "chat_turn"`?

Default trace name = **LangGraph** (not informative)

Now:

* Each trace is clearly labeled as **chat_turn**
* Easy to understand: 1 turn = 1 user-message + AI-reply

---

## 1Ô∏è‚É£3Ô∏è‚É£ Result After Thread Integration

LangSmith Dashboard Shows:

‚úÖ Projects
‚úÖ Traces
‚úÖ Threads section

Each thread:

* Represents one conversation
* Contains ordered turns
* Shows human ‚Üî AI messages cleanly

---

## 1Ô∏è‚É£4Ô∏è‚É£ Multiple Conversations Example

### Conversation 1

```
Hi
My name is Nitesh
Who created you?
```

‚û°Ô∏è Stored as **Thread 1**
‚û°Ô∏è Contains **3 traces**

---

### Conversation 2

```
Hi my name is Rahul
What is roadmap to study AI?
```

‚û°Ô∏è Stored as **Thread 2**
‚û°Ô∏è Completely separate & clean

---

## 1Ô∏è‚É£5Ô∏è‚É£ Benefits of Thread-based Observability

‚úÖ Clean conversation tracking
‚úÖ Easy debugging
‚úÖ Easy production monitoring
‚úÖ Understand user behavior
‚úÖ Analyze latency & cost per conversation

‚úÖ Essential for:

* Tools
* RAG
* MCP
* Multi-agent systems

---

## 1Ô∏è‚É£6Ô∏è‚É£ Why This Matters in Production

When chatbot is live:

* You cannot guess bugs
* You must observe behavior
* You must trace failures

üìå **LangSmith = Production-grade observability**

---

## üîö Final Summary

| Feature              | Status |
| -------------------- | ------ |
| UI                   | ‚úÖ      |
| Streaming            | ‚úÖ      |
| Persistence          | ‚úÖ      |
| Observability        | ‚úÖ      |
| Thread-based tracing | ‚úÖ      |

üöÄ **Your chatbot is now production-ready**

# üìò LangGraph Chatbot ‚Äì Adding Tools (Actions)

*(Simple English Notes with Code)*

---

## 1Ô∏è‚É£ Recap: What We Have Built So Far

We are building an **Agentic AI Chatbot using LangGraph**.

Till now, our chatbot has:

* ‚úÖ GUI (Streamlit UI)
* ‚úÖ Short-term memory
* ‚úÖ Database persistence
* ‚úÖ Streaming responses

### ‚ùå Current Limitation

The chatbot can **only talk**.

It cannot perform actions like:

* Calculations
* Internet search
* Fetch stock prices

---

## 2Ô∏è‚É£ Goal of This Video

üëâ **Add ACTION capabilities using Tools**

After this:

* Chatbot can chat normally
* Chatbot can perform tasks using tools

### Tools we will add:

1Ô∏è‚É£ Calculator tool (numerical calculations)

2Ô∏è‚É£ Internet search tool (DuckDuckGo)

3Ô∏è‚É£ Stock price tool (real-time price)

---

## 3Ô∏è‚É£ What Are Tools? (Simple Explanation)

A **Tool** is a function that the chatbot can call to perform real work.

Examples:

* Calculator ‚Üí math
* Search ‚Üí internet
* Stock API ‚Üí live prices

üìå The **LLM decides when to call a tool**.

---

## 4Ô∏è‚É£ High-Level Workflow (Before vs After)

### ‚ùå Old Workflow (Chat only)

```
START ‚Üí Chat Node ‚Üí END
```

### ‚úÖ New Workflow (Chat + Actions)

```
START ‚Üí Chat Node
          |
          |-- Normal chat ‚Üí END
          |
          |-- Tool needed ‚Üí Tool Node ‚Üí Chat Node ‚Üí END
```

---

## 5Ô∏è‚É£ Two NEW Important Concepts

### üîπ 1. ToolNode

**ToolNode** is a **pre-built LangGraph node**.

It:

* Holds all tools
* Executes the correct tool automatically

üìå Think of it as a **Tool Executor**.

---

### üîπ 2. tools_condition

`tools_condition` decides:

* Should we continue chatting?
* OR should we call a tool?

It creates a **conditional flow** in the graph.

---

## 6Ô∏è‚É£ Why Tool Output Must Go Back to LLM

### ‚ùå Problem (Without Loop)

* Tool returns raw JSON
* Output is not user-friendly
* Multi-step reasoning is not possible

Example:

```
"Apple stock price ‚Üí calculate cost of 50 shares"
```

‚ùå Impossible without a loop

---

### ‚úÖ Solution: Create a Loop

```
Chat Node ‚Üí Tool Node ‚Üí Chat Node ‚Üí Tool Node ‚Üí Chat Node ‚Üí END
```

With this loop, the LLM:

* Reads tool output
* Decides the next step
* Formats the final user-friendly answer

---

## 7Ô∏è‚É£ Required Imports

```python
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool

from langchain_community.tools import DuckDuckGoSearchRun
from dotenv import load_dotenv
```

---

## 8Ô∏è‚É£ Create the LLM

```python
load_dotenv()

llm = ChatOpenAI(model="gpt-4o-mini")
```

---

## 9Ô∏è‚É£ Define Tools

### üîπ Tool 1: DuckDuckGo Search (Prebuilt)

```python
search_tool = DuckDuckGoSearchRun()
```

---

### üîπ Tool 2: Calculator (Custom Tool)

```python
@tool
def calculator(a: int, b: int, operation: str) -> int:
    """
    Performs basic arithmetic operations between two numbers.
    operation can be add, subtract, multiply, divide
    """
    if operation == "add":
        return a + b
    elif operation == "subtract":
        return a - b
    elif operation == "multiply":
        return a * b
    elif operation == "divide":
        return a / b
    else:
        return "Invalid operation"
```

üìå **Docstring is VERY IMPORTANT** ‚Äì the LLM reads it to decide when to use this tool.


# üìò Agentic AI using LangGraph ‚Äì MCP (Model Context Protocol)

---

## 1Ô∏è‚É£ Playlist Status & Recap

This video resumes the **Agentic AI using LangGraph** playlist after a long pause.

### What we have covered so far (18 videos)

#### Foundations

* What is Agentic AI
* Difference between Generative AI vs Agentic AI
* LangChain vs LangGraph

#### LangGraph Core Concepts

* Nodes
* State
* Edges
* Execution flow

#### Workflow Types

* Sequential workflow
* Parallel workflow
* Conditional workflow
* Iterative (looping) workflow

#### Chatbot Project (Step-by-step)

* Basic LangGraph chatbot
* UI using Streamlit
* Streaming responses
* Resume chat (threads)
* Database persistence (SQLite)
* Observability using LangSmith
* Tools (Calculator, Web Search, Stock Price)

---

## 2Ô∏è‚É£ Today‚Äôs Topic: MCP (Model Context Protocol)

### Why this video exists

The playlist was paused mainly because the creator was working deeply on **MCP**, which has become very popular in the last **6 months**.

Now, **MCP is being integrated into the chatbot**.

---

## 3Ô∏è‚É£ What is MCP? (Very Simple Explanation)

**MCP = Improved and standardized way to connect tools with LLM applications**

You can think of MCP as:

> A better, cleaner, future‚Äëproof replacement for traditional *tool calling*

### Short definition

* MCP is **not a completely new idea**
* It is a **standardized way** to connect tools
* It clearly separates:

  * Tool logic
  * Client (chatbot) logic

---

## 4Ô∏è‚É£ Problem with Normal Tools (Important)

### Current tool approach

In LangChain / LangGraph, tools are:

* Library‚Äëbased tools (e.g., DuckDuckGo search)
* User‚Äëdefined tools (custom Python functions)

### Example tools already used

* Search tool
* Calculator tool
* Stock price tool

---

## 5Ô∏è‚É£ Example Problem: GitHub Integration

Suppose:

* Your chatbot already has **3 tools**
* Manager asks you to add **GitHub integration**
* Chatbot should answer questions about:

  * Pull Requests
  * Commits
  * Files

### What do you do today?

You must:

* Write custom GitHub tools
* Handle:

  * Authentication
  * Headers
  * API URLs
  * JSON parsing

### Example: GitHub Pull Request Tool (Conceptual)

```python
def get_pull_requests(owner, repo, state="open", limit=5):
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}"
    }
    url = f"https://api.github.com/repos/{owner}/{repo}/pulls"
    response = requests.get(url, headers=headers)
    return response.json()
```

---

## 6Ô∏è‚É£ The BIG Problem: Brittleness

### Why this approach is dangerous

* GitHub updates API versions
* URLs change
* JSON fields change
* Attribute names change

### Example changes

```
/pulls  ‚Üí /pull_requests
title   ‚Üí title_name
user    ‚Üí user_name
```

### üí• Result

* Chatbot crashes
* All tools break
* You must update code everywhere

---

## 7Ô∏è‚É£ Scaling Problem (N √ó M Explosion)

If:

* **N** tools
* **M** chatbots

Then:

```
Maintenance cost = N √ó M
```

If you integrate:

* GitHub
* Gmail
* Slack
* Jira

üëâ You spend more time **maintaining tools** than building features

---

## 8Ô∏è‚É£ How MCP Solves This

### Core MCP Idea: Separation of Concerns

| Client (Chatbot) | Server (Tools)      |
| ---------------- | ------------------- |
| Only config code | All tool logic      |
| Stable           | Can change          |
| No API logic     | Handles API changes |

### Key Rule

* Tool code stays on **MCP server**
* Client code **NEVER changes**

---

## 9Ô∏è‚É£ MCP Architecture (Simple)

### MCP Server

* Owns tool logic
* Handles APIs
* Handles version changes

### MCP Client

* Just connects via config
* Auto‚Äëdiscovers tools

---

## üîü From Tool Code ‚Üí MCP Config Code

### Old way (lots of code)

* Multiple Python functions
* API handling
* Error handling

### MCP way (only config)

```python
client = MultiServerMCPClient(
    servers={
        "math": {
            "transport": "stdio",
            "command": ["python", "main.py"]
        }
    }
)
```

‚úÖ No tool logic on client side

---

## 1Ô∏è‚É£1Ô∏è‚É£ Why Async is Required

### Important clarification

* MCP libraries are **async‚Äëonly**
* Therefore, your **LangGraph chatbot must be async**

---

## 1Ô∏è‚É£2Ô∏è‚É£ Converting LangGraph Chatbot to Async

### Key changes

#### 1. Make chat node async

```python
async def chat_node(state):
    response = await llm.ainvoke(state["messages"])
    return {"messages": [response]}
```

#### 2. Use `ainvoke` instead of `invoke`

```python
result = await chatbot.ainvoke(input_data)
```

#### 3. Async entry point

```python
if __name__ == "__main__":
    asyncio.run(main())
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ Why Async Matters (Intuition)

### Sequential (slow)

* Weather API ‚Üí wait
* Cricket score API ‚Üí wait

### Async (fast)

* Weather API ‚è≥
* Cricket score API ‚è≥
* Both run together

üëâ Faster, concurrent execution

---

## 1Ô∏è‚É£4Ô∏è‚É£ Building MCP Client in LangGraph

### Install library

```bash
pip install langchain-mcp-adapters
```

or

```bash
uv add langchain-mcp-adapters
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ Connecting to Local MCP Server (Math Server)

```python
from langchain_mcp_adapters.client import MultiServerMCPClient

client = MultiServerMCPClient(
    servers={
        "math": {
            "transport": "stdio",
            "command": ["python", "main.py"]
        }
    }
)
```

### Fetch tools from server

```python
tools = await client.get_tools()
llm = llm.bind_tools(tools)
```

‚úî Tools are auto‚Äëdiscovered
‚úî No manual tool definition

---

## 1Ô∏è‚É£6Ô∏è‚É£ Adding a Remote MCP Server (Expense Tracker)

```python
client = MultiServerMCPClient(
    servers={
        "math": {...},
        "expense": {
            "transport": "streamable_http",
            "url": "https://expense-mcp-server.com"
        }
    }
)
```

---

## 1Ô∏è‚É£7Ô∏è‚É£ What the Expense MCP Server Provides

* add_expense
* list_expenses
* summarize_expenses

üí° No expense logic written in chatbot
üí° Everything comes from MCP server

---

## 1Ô∏è‚É£8Ô∏è‚É£ Example Usage (No Client Code Change)

* Add an expense of ‚Çπ500 for Udemy course on 10th Nov
* List my expenses for November
* Summarize my expenses

All handled automatically via MCP üéØ

---

## 1Ô∏è‚É£9Ô∏è‚É£ Mixing Tools + MCP (Allowed)

You can:

* Use normal tools (search, stock)
* Use MCP servers (math, expense)

üëâ Mixing is allowed
üëâ MCP is recommended for future‚Äëproofing

---

## 2Ô∏è‚É£0Ô∏è‚É£ Real Project Notes (Important)

### Tech stack issues

* MCP ‚Üí async only
* LangGraph ‚Üí sync + async
* Streamlit ‚Üí mostly sync
* SQLite ‚Üí sync

### Fixes used

* Converted backend to async
* Used `aiosqlite` instead of `sqlite`
* Used async streaming (`astream`)

‚ö†Ô∏è **Not production‚Äëready**

### For production

* FastAPI backend
* React / Next.js frontend

---

## 2Ô∏è‚É£1Ô∏è‚É£ Why MCP Is the Future

* Industry standard
* Used by ChatGPT internally
* Stable
* Scalable
* No client maintenance

---

## 2Ô∏è‚É£2Ô∏è‚É£ What‚Äôs Next?

‚û°Ô∏è **Next video topic**

* RAG (Retrieval Augmented Generation)
* Ask questions on internal documents
* PDFs
* Knowledge bases

---

## ‚úÖ Final Takeaway

**Tools = brittle**
**MCP = robust, scalable, future‚Äëproof**

If you are building **serious agentic AI systems**, MCP is the right choice.


---

### üîπ Tool 3: Stock Price Tool (Custom API Tool)

```python
@tool
def get_stock_price(symbol: str) -> dict:
    """
    Returns the current stock price of a company using Alpha Vantage API.
    """
    import requests, os

    api_key = os.getenv("ALPHA_VANTAGE_API_KEY")
    url = (
        "https://www.alphavantage.co/query"
        f"?function=GLOBAL_QUOTE&symbol={symbol}&apikey={api_key}"
    )

    response = requests.get(url).json()
    return response
```

---

## üîü Bind Tools to LLM

```python
tools = [search_tool, calculator, get_stock_price]
llm_with_tools = llm.bind_tools(tools)
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Define State

```python
from typing import TypedDict, List
from langchain_core.messages import BaseMessage

class State(TypedDict):
    messages: List[BaseMessage]
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ Chat Node

```python
def chat_node(state: State):
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": state["messages"] + [response]}
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ Create ToolNode

```python
tool_node = ToolNode(tools)
```

---

## 1Ô∏è‚É£4Ô∏è‚É£ Build the Graph

```python
graph = StateGraph(State)

graph.add_node("chat", chat_node)
graph.add_node("tools", tool_node)

graph.add_edge(START, "chat")

graph.add_conditional_edges(
    "chat",
    tools_condition,
    {
        "tools": "tools",
        END: END
    }
)

# üîÅ IMPORTANT LOOP
graph.add_edge("tools", "chat")

chatbot = graph.compile()
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ Test the Workflow

### Normal Chat

```python
chatbot.invoke({
    "messages": [HumanMessage(content="Hello")]
})
```

### Calculator Tool

```python
chatbot.invoke({
    "messages": [HumanMessage(content="What is the product of 2 and 3?")]
})
```

### Stock Tool (Multi-step Reasoning)

```python
chatbot.invoke({
    "messages": [
        HumanMessage(content="What is the stock price of Apple and cost of 50 shares?")
    ]
})
```

---

## 1Ô∏è‚É£6Ô∏è‚É£ Why This Design Is Powerful

* ‚úÖ Clean, user-friendly output
* ‚úÖ Multi-step reasoning
* ‚úÖ Unlimited tools possible
* ‚úÖ True Agentic behavior

---

## 1Ô∏è‚É£7Ô∏è‚É£ Fix Streaming Issue (Frontend)

### ‚ùå Problem

Tool messages were also being streamed.

### ‚úÖ Solution: Stream only AI messages

```python
from langchain_core.messages import AIMessage

if isinstance(message, AIMessage):
    st.write_stream(message.content)
```

---

## 1Ô∏è‚É£8Ô∏è‚É£ Optional UX Improvement

* ‚úî Show status container in Streamlit
* ‚úî Display which tool is being used
* ‚úî Improves user trust & transparency

---

## 1Ô∏è‚É£9Ô∏è‚É£ Final Result

Your chatbot can now:

* Chat normally üß†
* Perform calculations üßÆ
* Search internet üåê
* Fetch live stock prices üìà
* Chain multiple tools automatically üîÅ

* # üìò LangGraph Chatbot ‚Üí RAG Chatbot (Simple English Notes)

---

## 1Ô∏è‚É£ Recap: What we have built so far

In the **Agentic AI using LangGraph** playlist, we are building **one chatbot as a running project**.

So far, we have added these features step by step:

* ‚úÖ Basic chatbot (LLM chat)
* ‚úÖ UI using Streamlit
* ‚úÖ Streaming responses (token by token)
* ‚úÖ Chat persistence (resume chat)
* ‚úÖ Observability using LangSmith
* ‚úÖ Tools (Calculator, Web Search, Stock Price)
* ‚úÖ MCP (Model Context Protocol)

Each new feature helped us learn **one new LangGraph concept**.

---

## 2Ô∏è‚É£ Goal of this Video: Convert Chatbot into a RAG Chatbot

### ‚ùå Before

* Chatbot could only answer using LLM knowledge
* Could NOT answer questions from your documents

### ‚úÖ After (RAG)

* Upload a PDF / document
* Ask questions based on that document
* Tools still work (calculator, stock price, etc.)

üìå **Result ‚Üí Multi-Utility Chatbot**

---

## 3Ô∏è‚É£ What is RAG? (Quick Recap)

**RAG = Retrieval Augmented Generation**

**Meaning:**

> Give extra context (documents) to the LLM while answering questions.

---

## 4Ô∏è‚É£ Why do we need RAG?

### üîπ Reason 1: Outdated Knowledge

* LLMs have a knowledge cutoff
* For latest info ‚Üí external data is needed
* ChatGPT itself uses RAG + Web Search

### üîπ Reason 2: Private Data

LLMs do NOT know:

* Your PDFs
* Company documents
* Personal files
* Internal reports

üìå **RAG allows:**

> Connecting private documents with LLM safely

### üîπ Reason 3: Hallucination

* LLMs sometimes give confident but wrong answers
* Example: fake research paper links

üìå **RAG helps by:**

* Forcing LLM to answer only from given context
* Reduces hallucination

---

## 5Ô∏è‚É£ Core Idea of RAG (Very Important)

### üîπ In-Context Learning

LLM answers based on:

* Your question
* Extra context (retrieved documents)
* Its own language ability

### ‚ùå Wrong approach

* Paste full book / full codebase ‚Üí ‚ùå Token limit exceeded

### ‚úÖ Correct approach

* Retrieve only relevant parts
* Send filtered context to LLM

---

## 6Ô∏è‚É£ High-Level RAG Architecture

### Step-by-step Flow

```
Document ‚Üí Split ‚Üí Embed ‚Üí Store ‚Üí Retrieve ‚Üí Prompt ‚Üí Answer
```

### Detailed Explanation

1Ô∏è‚É£ Load document (PDF, text, web page)
2Ô∏è‚É£ Split document into small chunks
3Ô∏è‚É£ Generate embeddings for each chunk
4Ô∏è‚É£ Store embeddings in a vector database
5Ô∏è‚É£ User asks a question
6Ô∏è‚É£ Question is embedded
7Ô∏è‚É£ Retriever finds most similar chunks
8Ô∏è‚É£ Retrieved chunks + question sent to LLM
9Ô∏è‚É£ LLM generates final answer

---

## 7Ô∏è‚É£ Libraries Used

* langchain
* langgraph
* langchain-openai
* faiss-cpu
* pypdf
* streamlit

---

## 8Ô∏è‚É£ Step 1: Load PDF Document

```python
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("intro_to_ml.pdf")
documents = loader.load()
```

---

## 9Ô∏è‚É£ Step 2: Split Document into Chunks

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = text_splitter.split_documents(documents)
```

üìå **Chunking is important to:**

* Maintain context
* Avoid token overflow

---

## üîü Step 3: Generate Embeddings

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Step 4: Store in Vector Database (FAISS)

```python
from langchain.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    chunks,
    embedding=embeddings
)
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ Step 5: Create Retriever

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)
```

üìå **Retriever returns top-4 most relevant chunks**

---

## 1Ô∏è‚É£3Ô∏è‚É£ Testing Retriever (Important Demo)

```python
docs = retriever.invoke("What is a decision tree?")
len(docs)
```

**Output ‚Üí 4**

Each item contains:

* page_content
* metadata

---

## 1Ô∏è‚É£4Ô∏è‚É£ Wrap RAG as a Tool (Very Important)

```python
from langchain.tools import tool

@tool
def rag_tool(query: str):
    """
    Use this tool when the user asks questions
    based on uploaded documents.
    """
    docs = retriever.invoke(query)

    context = [doc.page_content for doc in docs]
    metadata = [doc.metadata for doc in docs]

    return {
        "query": query,
        "context": context,
        "metadata": metadata
    }
```

üìå **RAG is treated as just another tool**

---

## 1Ô∏è‚É£5Ô∏è‚É£ Bind Tools to LLM

```python
tools = [rag_tool, calculator_tool, stock_price_tool]

llm = llm.bind_tools(tools)
```

---

## 1Ô∏è‚É£6Ô∏è‚É£ LangGraph Nodes

### üîπ Chat Node

* Decides whether a tool is needed

### üîπ Tool Node

* Executes RAG tool

```python
from langgraph.graph import StateGraph

graph = StateGraph(ChatState)

graph.add_node("chat", chat_node)
graph.add_node("tools", tool_node)

graph.add_edge("chat", "tools")
graph.add_edge("tools", "chat")

graph.set_entry_point("chat")

app = graph.compile()
```

---

## 1Ô∏è‚É£7Ô∏è‚É£ Full Chat Flow

```
User Question
   ‚Üì
Chat Node (decision)
   ‚Üì
RAG Tool (retrieve docs)
   ‚Üì
Chat Node (final answer)
   ‚Üì
End
```

---

## 1Ô∏è‚É£8Ô∏è‚É£ Example Query

```python
app.invoke({
    "messages": [
        ("human", "Using the PDF, explain how to split a node in a decision tree")
    ]
})
```

‚è±Ô∏è Takes **~8‚Äì9 seconds**
(because retrieval + LLM processing)

---

## 1Ô∏è‚É£9Ô∏è‚É£ Observability with LangSmith

LangSmith shows:

* Chat node execution
* Tool invocation
* Retrieved chunks
* Final response

üìå **Visual Flow:**

```
Chat ‚Üí Tool ‚Üí Chat
```

---

## 2Ô∏è‚É£0Ô∏è‚É£ Integrating RAG into Existing Project

### üÜï New Files Added

* `langgraph_rag_backend.py`
* `streamlit_rag_frontend.py`

### üîπ Backend Changes

* Created `ingest_pdf()` function
* Added RAG tool with other tools

### üîπ Frontend Changes

* PDF upload option in sidebar
* Minor thread handling updates

---

## 2Ô∏è‚É£1Ô∏è‚É£ Key Takeaway

* ‚úÖ RAG in LangGraph is very simple
* ‚úÖ Treat RAG as a tool
* ‚úÖ Same agent architecture
* ‚úÖ Production-ready design

---

## 2Ô∏è‚É£2Ô∏è‚É£ What‚Äôs Next?

* More LangGraph concepts
* Advanced agents
* Agent orchestration
* Real-world Agentic AI systems

---

## üéØ Final Summary (One Line)

> **If you know tools in LangGraph, RAG is just another tool.**
>
> üìò Human-in-the-Loop (HITL) in Agentic AI using LangGraph

(Simple English Notes with Examples & Code)

1Ô∏è‚É£ What is this video about?

In this video, we learn a very important concept used in real-world Agentic AI systems:

üëâ HITL ‚Äì Human in the Loop

This concept becomes mandatory when you build serious AI agents like:

Chatbots

Payment agents

Booking systems

Customer support bots

Tool-using AI systems

2Ô∏è‚É£ What is HITL (Human in the Loop)?

Simple definition:

HITL is a design approach where a human participates at important decision points in an AI workflow.

Instead of letting AI do everything automatically, we pause, ask a human, and continue only after approval.

In short:

AI works autonomously ‚úÖ

But critical decisions need human approval ‚úÖ

3Ô∏è‚É£ Why Agentic AI Needs Human Oversight

Agentic AI systems are created for autonomy.

Example: Customer Support

Companies like:

Swiggy

Zomato

Amazon

Receive thousands of repetitive requests:

‚ÄúOrder not delivered‚Äù

‚ÄúRefund not received‚Äù

These can be handled by AI agents ‚Üí saves human effort

‚ùå Problem with Full Autonomy

Current LLMs are not perfect:

They can misunderstand user intent

They can hallucinate

They can make wrong assumptions

So we cannot fully trust AI for:

Payments

Bookings

Deleting data

Sending important emails

üëâ That‚Äôs why HITL is needed

4Ô∏è‚É£ Real-World HITL Example ‚Äì Flight Booking
Without HITL ‚ùå

User:

‚ÄúBook my flight ticket‚Äù

AI:

Searches flights

Selects one

Books it

Makes payment ‚ùå

‚ö†Ô∏è Risky and dangerous

With HITL ‚úÖ

Flow:

AI searches flights

AI shows best options

AI asks:

‚ÄúShould I book this flight?‚Äù

Human confirms

Ticket is booked

üëâ Human judgment controls final action

5Ô∏è‚É£ Core Reasons Why HITL Exists
üîπ Reason 1: Accuracy

LLMs can:

Misinterpret queries

Fail on ambiguous inputs

Example:

User says:

‚ÄúBook flight for next Friday‚Äù

Ambiguity:

This Friday?

Next week‚Äôs Friday?

üëâ HITL asks:

‚ÄúWhich Friday do you mean?‚Äù

üîπ Reason 2: Accountability (Very Important)

AI cannot be blamed
Humans can be held responsible

Example: Gmail Smart Reply

AI generates email reply

Gmail asks user to review

User approves

Then email is sent

üëâ Accountability stays with the human

6Ô∏è‚É£ Benefits of HITL in Agentic Systems
‚úÖ 1. Better Accuracy

Human corrects AI mistakes

‚úÖ 2. Improved Safety

Example:

AI wants to delete files

Human confirms first

‚úÖ 3. Ethical Alignment

AI may generate:

Cold replies

Emotionless responses

Human can add:

Empathy

Tone

Company values

‚úÖ 4. Better User Experience

Human + AI synergy = best output

7Ô∏è‚É£ Common HITL Patterns in AI Systems
üîπ 1. Action Approval Pattern (Most Common)

Human approval before:

Payments

Sending emails

Deleting files

Booking tickets

üîπ 2. Output Review / Edit Pattern

Example:

Blog writing agent

Research agent

Flow:

AI generates draft

Human reviews

Approves or edits

Then publishes

üîπ 3. Ambiguity Clarification Pattern

If AI is confused:

It asks the human

Example:

‚ÄúDo you mean this Friday or next Friday?‚Äù

üîπ 4. Escalation Pattern

AI gives up and hands control to human

Example:

Customer support chatbot

‚ÄúWould you like to talk to a human agent?‚Äù

8Ô∏è‚É£ HITL from a LangGraph Perspective

LangGraph supports HITL using two core concepts:

üîπ 1. interrupt()

Pauses graph execution

Sends message to frontend

Saves current state

üîπ 2. Command(resume=...)

Resumes execution

Continues from same node

Uses human input

9Ô∏è‚É£ Conceptual HITL Workflow (LangGraph)

Example: Social Media Tweet Agent

Workflow:

START
 ‚Üí Research Tweet
 ‚Üí Generate Draft
 ‚Üí ‚ùó INTERRUPT (Ask Human)
 ‚Üí Post Tweet
 ‚Üí END

üîü What Happens During interrupt()?

When LangGraph hits interrupt():

Execution pauses

Current state is saved (via checkpointer)

Message is sent to frontend

Human responds

Graph resumes from same point

1Ô∏è‚É£1Ô∏è‚É£ Basic HITL Code Example (Simple)
Scenario:

Before sending a question to LLM, ask human:

‚ÄúDo you really want to ask this?‚Äù

üîπ HITL Chat Node
from langgraph.types import interrupt
from langchain.schema import AIMessage

def chat_node(state):
    decision = interrupt({
        "type": "approval",
        "reason": "User wants to ask LLM a question",
        "question": state["messages"][-1].content,
        "instruction": "Approve or reject (yes/no)"
    })

    if decision["approved"] == "no":
        return {"messages": [AIMessage(content="Not approved")]}

    response = llm.invoke(state["messages"])
    return {"messages": [response]}

üîπ Graph Definition
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

graph = StateGraph(ChatState)

graph.add_node("chat", chat_node)
graph.add_edge("start", "chat")
graph.add_edge("chat", END)

checkpointer = MemorySaver()
app = graph.compile(checkpointer=checkpointer)

üîπ First Invoke (Pause Happens)
result = app.invoke(
    {"messages": [HumanMessage(content="Explain gradient descent")]},
    config={"thread_id": "1"}
)


You will receive an interrupt message.

üîπ Resume After Human Input
from langgraph.types import Command

result = app.invoke(
    Command(resume={"approved": "yes"}),
    config={"thread_id": "1"}
)


Graph resumes from same node üéØ

1Ô∏è‚É£2Ô∏è‚É£ Advanced Example ‚Äì HITL in Tool-Using Chatbot
Tools:

get_stock_price

purchase_stock (risky action)

‚ùå Without HITL

User:

‚ÄúPurchase 10 stocks‚Äù

AI:

Directly buys ‚ùå

No confirmation

No accountability

‚úÖ With HITL

AI asks:

‚ÄúApprove buying 10 shares of Apple? (yes/no)‚Äù

Only proceeds after approval

üîπ HITL Inside Tool
def purchase_stock(company, quantity):
    decision = interrupt(
        f"Approve buying {quantity} shares of {company}? (yes/no)"
    )

    if decision.lower() != "yes":
        return "Purchase cancelled"

    return f"Successfully purchased {quantity} shares of {company}"

üîπ Frontend / CLI Logic
if interrupt_msg:
    print(interrupt_msg)
    user_decision = input("yes/no: ")

    app.invoke(
        Command(resume=user_decision),
        config={"thread_id": thread_id}
    )

1Ô∏è‚É£3Ô∏è‚É£ Key Takeaways

HITL is mandatory for real-world AI

LangGraph supports HITL natively

Core concepts:

interrupt()

Command(resume=...)

HITL improves:

Accuracy

Safety

Accountability

User trust

‚úÖ Final Summary

HITL = AI power + Human judgment

LangGraph makes HITL:

Simple

Intuitive

Production-ready

You‚Äôll use this a lot in:

RAG systems

Tool-using agents

Finance apps

Enterprise chatbots

# üìò Subgraphs in Agentic AI using LangGraph

(Simple English Notes with Code)

---

## 1Ô∏è‚É£ Introduction: What are Subgraphs?

Till now, we have learned that:

* Any AI workflow can be represented as a **Graph**
* A graph contains:

  * **Nodes** ‚Üí tasks (LLM call, tool call, retrieval, etc.)
  * **Edges** ‚Üí execution flow

### üîπ Core Idea of Subgraphs

üëâ A **Subgraph** is a graph inside another graph.

Normally:

* One node = one task

With subgraphs:

* One node itself can be a **complete graph**

üìå **Definition (Simple)**

A Subgraph is a graph that is embedded and executed as a node inside another graph.

* **Big Graph** = Parent Graph
* **Small internal Graph** = Subgraph

---

## 2Ô∏è‚É£ Why Subgraphs Are Needed in Real GenAI Systems

### üîπ Simple GenAI App (Beginner Level)

```
User Question ‚Üí LLM ‚Üí Answer
```

Simple and easy.

### üîπ Real-World GenAI Apps (Advanced Level)

Real systems usually include:

* Tools
* RAG (Retrieval Augmented Generation)
* Conditional routing
* Retry logic
* Memory
* HITL (Human-in-the-loop)
* Evaluation
* Guardrails
* Deployment & Monitoring

üëâ All this makes the graph **very complex**.

---

## 3Ô∏è‚É£ Real Example: Software Development Agent

Imagine building an **AI Software Development Agent**.

Tasks involved:

* Planning (Team Lead)
* Backend Development
* Frontend Development
* Testing
* Code Review
* DevOps (Deploy & Monitor)

If everything is placed inside **one graph**:

‚ùå Too complex
‚ùå Hard to debug
‚ùå Hard to maintain

---

## 4Ô∏è‚É£ Solution: Multi-Agent Architecture using Subgraphs

Break one big agent into multiple small agents.

| Agent          | Role               |
| -------------- | ------------------ |
| Planning Agent | Planning           |
| Coding Agent   | Backend / Frontend |
| Testing Agent  | Testing            |
| Review Agent   | Code Review        |
| DevOps Agent   | Deployment         |

üëâ Each agent = **One Subgraph**

Each subgraph has:

* Its own tools
* Its own memory
* Its own retry logic
* Its own guardrails

---

## 5Ô∏è‚É£ Core Benefits of Subgraphs

### ‚úÖ 1. Modularity

* Break big systems into small logical units
* Works like **functions** in programming

### ‚úÖ 2. Reusability

* Same Coding Agent can be reused for:

  * Backend
  * Frontend

### ‚úÖ 3. Maintainability

* Easier debugging
* Easier upgrades
* Easier testing

---

## 6Ô∏è‚É£ LangGraph-Specific Benefits of Subgraphs

### üîπ 1. Failure Isolation

* If one subgraph fails ‚Üí others continue
* One failure does **NOT** crash the whole system

### üîπ 2. State Separation

Without subgraphs:

* One giant shared state ‚ùå
* State conflicts ‚ùå

With subgraphs:

* Each subgraph has its own state
* No state pollution

### üîπ 3. Observability (Tracing)

Using **LangSmith**:

* Trace each subgraph separately
* Measure:

  * Token usage
  * Latency
  * Errors per agent

üìå Example:

* Trace Coding Agent separately
* Trace Testing Agent separately

---

## 7Ô∏è‚É£ Ways to Implement Subgraphs in LangGraph

LangGraph provides **two mechanisms**:

### üîπ Method 1: Invoke a Subgraph from a Node

* Parent graph and subgraph have **separate states**
* Subgraph is invoked inside a node function
* High isolation

### üîπ Method 2: Add Subgraph as a Node

* Subgraph itself becomes a node
* Parent and subgraph **share the same state**
* Simple design

---

## 8Ô∏è‚É£ Practical Example: English ‚Üí Hindi Translation

### üß† Use Case

1. User asks a question
2. LLM generates answer in English
3. Another LLM translates it into Hindi

---

## ‚úÖ METHOD 1: Subgraph with Isolated State

### Step 1Ô∏è‚É£ Translation Subgraph

#### Subgraph State

```python
from typing import TypedDict

class TranslationState(TypedDict):
    input_text: str
    translated_text: str
```

#### Translation Node

```python
def translate_node(state: TranslationState):
    prompt = f"Translate this text to Hindi:\n{state['input_text']}"
    response = llm.invoke(prompt)

    return {
        "translated_text": response.content
    }
```

#### Build Translation Subgraph

```python
from langgraph.graph import StateGraph, START, END

translation_graph = StateGraph(TranslationState)

translation_graph.add_node("translate", translate_node)
translation_graph.add_edge(START, "translate")
translation_graph.add_edge("translate", END)

translation_subgraph = translation_graph.compile()
```

---

### Step 2Ô∏è‚É£ Parent Graph

#### Parent State

```python
class ParentState(TypedDict):
    question: str
    english_answer: str
    hindi_answer: str
```

#### Generate Answer Node

```python
def generate_answer(state: ParentState):
    response = llm.invoke(state["question"])
    return {"english_answer": response.content}
```

#### Translate Node (Invokes Subgraph)

```python
def translate_answer(state: ParentState):
    result = translation_subgraph.invoke({
        "input_text": state["english_answer"]
    })

    return {
        "hindi_answer": result["translated_text"]
    }
```

#### Build Parent Graph

```python
parent_graph = StateGraph(ParentState)

parent_graph.add_node("generate", generate_answer)
parent_graph.add_node("translate", translate_answer)

parent_graph.add_edge(START, "generate")
parent_graph.add_edge("generate", "translate")
parent_graph.add_edge("translate", END)

app = parent_graph.compile()
```

#### Run

```python
result = app.invoke({
    "question": "What is Artificial Intelligence?"
})

print(result)
```

---

## ‚úÖ METHOD 2: Subgraph as a Node (Shared State)

### Shared State

```python
class SharedState(TypedDict):
    question: str
    english_answer: str
    hindi_answer: str
```

### Subgraph (Uses Shared State)

```python
def translate_node(state: SharedState):
    prompt = f"Translate this to Hindi:\n{state['english_answer']}"
    response = llm.invoke(prompt)

    return {"hindi_answer": response.content}
```

### Build Translation Subgraph

```python
translation_graph = StateGraph(SharedState)
translation_graph.add_node("translate", translate_node)
translation_graph.add_edge(START, "translate")
translation_graph.add_edge("translate", END)

translation_subgraph = translation_graph.compile()
```

### Parent Graph (Subgraph as Node)

```python
parent_graph = StateGraph(SharedState)

parent_graph.add_node("generate", generate_answer)
parent_graph.add_node("translator", translation_subgraph)

parent_graph.add_edge(START, "generate")
parent_graph.add_edge("generate", "translator")
parent_graph.add_edge("translator", END)

app = parent_graph.compile()
```

---

## 9Ô∏è‚É£ Comparison Summary

| Feature    | Method 1 | Method 2 |
| ---------- | -------- | -------- |
| State      | Separate | Shared   |
| Isolation  | High     | Low      |
| Simplicity | Medium   | High     |
| Control    | More     | Less     |

---

## üîü Best Practices & Extra Features

### ‚úÖ Persistence

* Add **checkpointer only to the parent graph**
* Subgraphs automatically inherit persistence

### ‚úÖ Streaming

* Subgraphs fully support streaming outputs

### ‚úÖ Observability

* Each subgraph can be traced independently in **LangSmith**

---

## üîö Final Takeaway

Subgraphs are **mandatory** for complex Agentic AI systems.

They enable:

* Multi-agent architecture
* Clean design
* Scalable workflows

üëâ **Future AI systems = Graphs of Graphs** üöÄ


üìò  Why Memory Matters in GenAI

( First Principles Approach)

1Ô∏è‚É£ Why Memory is Extremely Important in GenAI

If you want to build real GenAI or Agentic AI applications, memory is not optional.

Think about:

Chatbots (ChatGPT, Gemini)

AI agents

Personal assistants

‚ùå Without memory:

AI forgets everything

Conversations break

User experience becomes frustrating

üëâ Conclusion:
No GenAI system can work properly without memory.

2Ô∏è‚É£ How LLMs Work at Inference (Very Important Foundation)
üîπ LLMs are Stateless by Design

At inference time, an LLM behaves like a pure mathematical function:

ùë¶
=
ùëì
ùúÉ
(
ùë•
)
y=f
Œ∏
	‚Äã

(x)

Where:

x = input tokens (your prompt)

Œ∏ (theta) = model parameters (fixed after training)

y = output tokens (LLM response)

üîπ Key Observations

Œ∏ is fixed (trained earlier, user cannot change it)

x changes every time (new prompt)

Output depends only on current input

üìå Important Result:
üëâ LLMs do NOT remember past interactions

3Ô∏è‚É£ What Does ‚ÄúStateless‚Äù Mean?

Stateless system =
Output depends only on current input, not on past inputs.

Example:
User: My name is Nitesh
LLM: Nice to meet you, Nitesh

User: What is my name?
LLM: Sorry, I don‚Äôt know your name


üí° Why?

Second call does not know about the first call

Each call is independent

üëâ LLMs have NO intrinsic memory

4Ô∏è‚É£ The Core Problem
‚ùå Fact 1: LLMs have no memory
‚ùå Fact 2: Almost every GenAI app needs memory

‚ö†Ô∏è This creates a deadlock

So what do we do?

üëâ We build memory externally around the LLM

5Ô∏è‚É£ Context Window (Very Important Concept)
üîπ What is Context Window?

Context Window =
Maximum number of tokens an LLM can read at one time before answering.

üì∑ Camera Analogy:

LLM = Camera

Context Window = Lens

Bigger lens ‚Üí sees more scene

Examples:

128k tokens ‚âà ~200 pages

Gemini models ‚Üí up to 1 million tokens

üìå This is powerful because:
üëâ We can send large conversation history to the model

6Ô∏è‚É£ In-Context Learning

LLMs have two sources of knowledge:

1Ô∏è‚É£ Parametric Knowledge

Learned during training

Stored in parameters

2Ô∏è‚É£ In-Context Knowledge

Provided inside the prompt itself

Example:

Upload a 100-page private PDF

Ask questions from it

Model answers using prompt content, not training data

üìå This ability is called In-Context Learning

7Ô∏è‚É£ First Memory Solution: Short-Term Memory (STM)
üîπ Idea

Since LLMs are stateless, we:
üëâ Send conversation history with every request

Formula:
ùë¶
2
=
ùëì
ùúÉ
(
ùë•
1
,
ùë¶
1
,
ùë•
2
)
y
2
	‚Äã

=f
Œ∏
	‚Äã

(x
1
	‚Äã

,y
1
	‚Äã

,x
2
	‚Äã

)

Instead of:

ùë¶
2
=
ùëì
ùúÉ
(
ùë•
2
)
y
2
	‚Äã

=f
Œ∏
	‚Äã

(x
2
	‚Äã

)
8Ô∏è‚É£ Code Example: Short-Term Memory
messages = []

# First user message
messages.append("My name is Nitesh")
response1 = llm.invoke(messages)
messages.append(response1)

# Second user message
messages.append("What is my name?")
response2 = llm.invoke(messages)

print(response2)
# Output: Your name is Nitesh

üîπ What changed?

messages acts as state

System becomes stateful

üìå This is called:

Conversation Buffer

Short-Term Memory

9Ô∏è‚É£ Why It Is Called Short-Term Memory

Stored in RAM

Lost when:

App restarts

Server crashes

New conversation starts

üëâ Exists only inside one conversation

üîü How Chatbots Use Short-Term Memory
üîπ Conversation = Thread

Each chat = one thread

STM is thread-scoped

When:

New chat starts ‚Üí memory resets

Old chat resumes ‚Üí memory reloaded

üìå That‚Äôs why ChatGPT shows separate conversations.

1Ô∏è‚É£1Ô∏è‚É£ Limitations of Short-Term Memory
‚ùå Problem 1: Fragile

Server crash ‚Üí memory lost

‚ùå Problem 2: Context Window Overflow

Long chats exceed token limits

Model becomes incoherent or hallucinates

‚úÖ Solutions:

Trimming: Keep last N messages

Summarization: Summarize old messages + recent messages

‚ùå Problem 3: Thread-Scoped (BIGGEST ISSUE)

STM cannot:

Remember user preferences across chats

Learn over time

Personalize the assistant

Do cross-conversation reasoning

üìå Result:

Every new chat = user becomes a stranger again

1Ô∏è‚É£2Ô∏è‚É£ Why We Need Long-Term Memory (LTM)

We need a new type of memory that:

‚úÖ Survives across conversations
‚úÖ Stores important information for days/months
‚úÖ Enables personalization
‚úÖ Compounds learning over time

üëâ This is Long-Term Memory

1Ô∏è‚É£3Ô∏è‚É£ What Goes Into Long-Term Memory?

Only important, stable, reusable information.

Examples:

User preferences (Python over Java)

User profile (developer, beginner)

Past decisions and outcomes

What worked / what failed

‚ùå Not raw chat logs

1Ô∏è‚É£4Ô∏è‚É£ Types of Long-Term Memory
üß† 1. Episodic Memory

Past events

What happened before?

What worked / failed?

üß† 2. Semantic Memory

Facts

User prefers Python

Budget = ‚Çπ10,000

System uses PostgreSQL

üß† 3. Procedural Memory

How to do things

Preferred workflows

Rules

Strategies

Learned behaviors

üìå This makes agents feel smarter over time

1Ô∏è‚É£5Ô∏è‚É£ How Long-Term Memory Works (High Level)
4-Step Pipeline

1Ô∏è‚É£ Creation

Detect memory-worthy information

2Ô∏è‚É£ Storage

Save in durable storage (DB, vector DB, KV store)

3Ô∏è‚É£ Retrieval

Fetch relevant memories for current situation

4Ô∏è‚É£ Injection

Inject retrieved memory into short-term memory

Then send to LLM

üìå LTM never talks directly to LLM
üëâ Always passes through STM

1Ô∏è‚É£6Ô∏è‚É£ Challenges in Building Memory Systems

Deciding what to remember

Retrieving right memory at right time

Engineering complexity

Multiple storage systems

1Ô∏è‚É£7Ô∏è‚É£ Tools & Libraries for Memory

Popular solutions:

LangMem (LangChain ecosystem)

Mem0

Supermemory

üëâ These manage memory so developers can focus on apps

1Ô∏è‚É£8Ô∏è‚É£ Future of Memory in LLMs

Research ongoing for intrinsic memory

Google research: Titans + Mirage

New transformer architectures with built-in memory

üìå Because:

Without memory, GenAI and Agentic AI are impossible

‚úÖ Final Takeaway
Memory Type	Scope	Purpose
Short-Term	One conversation	Continuity
Long-Term	Across conversations	Personalization & learning

üëâ Real AI = LLM + Short-Term Memory + Long-Term Memory









